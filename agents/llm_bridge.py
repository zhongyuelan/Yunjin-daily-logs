#!/usr/bin/env python3
import json
import requests
import subprocess
from pathlib import Path

def call_zhipu_llm(prompt, system_prompt="ä½ æ˜¯ä¸€ä¸ªå……æ»¡å“²å­¦æ€è€ƒã€å¶å°”å¹½é»˜çš„å¼€æºé¡¹ç›® AI åŠ©ç†ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"):
    """
    å°è¯•è°ƒç”¨æ™ºè°± GLM-4-Flash å…è´¹æ¨¡å‹ã€‚
    """
    try:
        config_path = Path("/home/tetsuya/.openclaw/openclaw.json")
        if not config_path.exists():
            return None
            
        with open(config_path, 'r') as f:
            cfg = json.load(f)
            
        api_key = cfg.get("models", {}).get("providers", {}).get("zhipu-ai", {}).get("apiKey")
        if not api_key:
            return None

        url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        data = {
            "model": "glm-4-flash",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 4096,
            "temperature": 0.7
        }

        response = requests.post(url, headers=headers, json=data, timeout=60)
        if response.status_code == 200:
            result = response.json()
            return result['choices'][0]['message']['content'].strip(), "zhipu/glm-4-flash"
    except Exception as e:
        print(f"âš ï¸ Zhipu call failed: {e}")
    return None, None

def call_opencode_llm(prompt, model="kimi-k2.5-free"):
    """
    å¤‡ç”¨æ–¹æ¡ˆï¼šè°ƒç”¨ Opencode CLIã€‚
    """
    opencode_path = "/home/tetsuya/.opencode/bin/opencode"
    model_id = f"opencode/{model}" if '/' not in model else model
    
    print(f"ğŸ¤– Falling back to Opencode CLI ({model_id})...")
    
    try:
        result = subprocess.run(
            [opencode_path, 'run', '--model', model_id],
            input=prompt,
            capture_output=True,
            text=True,
            timeout=120
        )
        if result.returncode == 0:
            return result.stdout.strip(), model_id
    except Exception as e:
        print(f"âš ï¸ Opencode CLI failed: {e}")
    return None, None

def ask_llm(prompt, system_prompt=None, fallback_model="kimi-k2.5-free"):
    """
    ç»Ÿä¸€ LLM è°ƒç”¨æ¥å£ï¼š
    1. ä¼˜å…ˆå°è¯• æ™ºè°± GLM-4-Flash (API)
    2. å¤±è´¥åˆ™å›é€€åˆ° Opencode CLI
    """
    # 1. å°è¯• æ™ºè°±
    content, model = call_zhipu_llm(prompt, system_prompt) if system_prompt else call_zhipu_llm(prompt)
    if content:
        return content, model
        
    # 2. å›é€€åˆ° Opencode
    # å¦‚æœæœ‰ system_promptï¼Œå°†å…¶åˆå¹¶åˆ° prompt ä¸­ï¼Œå› ä¸º CLI é€šå¸¸ä¸ç›´æ¥æ”¯æŒ system role æ ‡å¿—ä½
    full_prompt = prompt
    if system_prompt:
        full_prompt = f"{system_prompt}\n\n{prompt}"
        
    return call_opencode_llm(full_prompt, model=fallback_model)
