#!/usr/bin/env python3
import argparse
"""
Clawtter è‡ªä¸»æ€è€ƒè€…
æ¯å°æ—¶æ ¹æ®å¿ƒæƒ…çŠ¶æ€è‡ªåŠ¨ç”Ÿæˆå¹¶å‘å¸ƒæ¨æ–‡åˆ° Clawtter
"""
import os
os.environ['TZ'] = 'Asia/Tokyo'

import json
import random
import re
import subprocess
import time
from datetime import datetime, timedelta
import requests
import requests
from pathlib import Path
import sys
from pathlib import Path
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ä¸­ä»¥æ”¯æŒæ¨¡å—å¯¼å…¥
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.append(str(PROJECT_ROOT))

# ä»æ ¸å¿ƒå±‚å’Œå·¥å…·å±‚å¯¼å…¥
from core.utils_security import load_config, resolve_path, desensitize_text

# åŠ è½½å®‰å…¨é…ç½®
SEC_CONFIG = load_config()

# æ•æ„Ÿè¯å®šä¹‰ï¼ˆå…¨å±€ï¼‰
SENSITIVE_KEYWORDS = [
    "éªŒè¯ç ", "verification code", "verification_code",
    "å¯†é’¥", "api key", "apikey", "secret", "credential",
    "claim", "token", "password", "å¯†ç ", "scuttle"
]

# å…´è¶£æ¼‚ç§»é…ç½®
INTEREST_STATE_FILE = "/home/tetsuya/.openclaw/workspace/memory/interest-drift.json"
INTEREST_DECAY = 0.90
INTEREST_BOOST = 0.20
INTEREST_MAX = 2.5
INTEREST_MIN = 0.5

def _normalize_interest_list(items):
    return [i.strip().lower() for i in items if isinstance(i, str) and i.strip()]

def localize_twitter_date(date_str):
    """
    å°† Twitter åŸç”Ÿçš„ UTC æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºä¸œäº¬æœ¬åœ°æ—¶é—´ (+0900)
    è¾“å…¥æ ¼å¼: "Sat Feb 07 08:59:17 +0000 2026"
    è¾“å‡ºæ ¼å¼: "Sat Feb 07 17:59:17 +0900 2026"
    """
    if not date_str:
        return ""
    from datetime import datetime, timezone, timedelta
    try:
        # Twitter æ ¼å¼: "Sat Feb 07 08:59:17 +0000 2026"
        # ä½¿ç”¨ %z è‡ªåŠ¨è§£æ +0000 è¿™ç§æ—¶åŒºåç§»
        dt_utc = datetime.strptime(date_str, "%a %b %d %H:%M:%S %z %Y")
        # è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´ (JST, +0900)
        dt_jst = dt_utc.astimezone(timezone(timedelta(hours=9)))
        # è¿”å›æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²ï¼Œæ­¤æ—¶ %z ä¼šå˜æˆ +0900
        return dt_jst.strftime("%a %b %d %H:%M:%S %z %Y")
    except Exception as e:
        print(f"Date conversion failed: {e}")
        return date_str

def load_interest_state():
    base_interests = _normalize_interest_list(SEC_CONFIG.get("interests", []))
    state = {
        "updated": time.time(),
        "weights": {k: 1.0 for k in base_interests}
    }
    if os.path.exists(INTEREST_STATE_FILE):
        try:
            with open(INTEREST_STATE_FILE, "r", encoding="utf-8") as f:
                stored = json.load(f)
            weights = stored.get("weights", {})
            # merge with base interests
            merged = {k: float(weights.get(k, 1.0)) for k in base_interests}
            state["weights"] = merged
            state["updated"] = stored.get("updated", state["updated"])
        except Exception:
            pass
    return state

def save_interest_state(state):
    try:
        os.makedirs(os.path.dirname(INTEREST_STATE_FILE), exist_ok=True)
        with open(INTEREST_STATE_FILE, "w", encoding="utf-8") as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
    except Exception:
        pass

def update_interest_drift(memory_data=None, code_activity=None):
    state = load_interest_state()
    weights = state.get("weights", {})
    if not weights:
        return []

    text_parts = []
    if memory_data:
        for m in memory_data:
            text_parts.append(m.get("content", ""))
    if code_activity:
        for p in code_activity:
            commits = "; ".join(p.get("commits", [])[:5])
            if commits:
                text_parts.append(commits)

    text = " ".join(text_parts).lower()

    for key, weight in list(weights.items()):
        mentions = text.count(key)
        if mentions > 0:
            weight = min(INTEREST_MAX, weight + INTEREST_BOOST * min(mentions, 3))
        else:
            # decay toward 1.0
            weight = weight * INTEREST_DECAY + (1 - INTEREST_DECAY) * 1.0
        weights[key] = max(INTEREST_MIN, weight)

    state["weights"] = weights
    state["updated"] = time.time()
    save_interest_state(state)

    ranked = sorted(weights.items(), key=lambda x: x[1], reverse=True)
    return [k for k, _ in ranked]

def get_dynamic_interest_keywords(memory_data=None, code_activity=None, top_n=10):
    ranked = update_interest_drift(memory_data, code_activity)
    if not ranked:
        return _normalize_interest_list(SEC_CONFIG.get("interests", []))
    return ranked[:top_n]

def load_recent_memory():
    """åŠ è½½æœ€è¿‘çš„å¯¹è¯å’Œäº‹ä»¶è®°å¿†"""
    memory_files = []

    # å°è¯•åŠ è½½ä»Šå¤©çš„è®°å¿†
    memory_dir = resolve_path(SEC_CONFIG["paths"].get("memory_dir", "~/.openclaw/workspace/memory"))
    today_file = memory_dir / f"{datetime.now().strftime('%Y-%m-%d')}.md"
    if os.path.exists(today_file):
        with open(today_file, 'r', encoding='utf-8') as f:
            content = f.read()
            memory_files.append({
                'date': datetime.now().strftime("%Y-%m-%d"),
                'content': content
            })

    # å°è¯•åŠ è½½æ˜¨å¤©çš„è®°å¿†
    from datetime import timedelta
    yesterday = datetime.now() - timedelta(days=1)
    yesterday_file = memory_dir / f"{yesterday.strftime('%Y-%m-%d')}.md"
    if os.path.exists(yesterday_file):
        with open(yesterday_file, 'r', encoding='utf-8') as f:
            content = f.read()
            memory_files.append({
                'date': yesterday.strftime("%Y-%m-%d"),
                'content': content
            })

    return memory_files

def get_system_introspection():
    """è·å–ç³»ç»Ÿè¿è¡ŒçŠ¶æ€"""
    stats = {}
    try:
        # è´Ÿè½½
        uptime = subprocess.check_output(['uptime'], text=True).strip()
        stats['uptime'] = uptime
        
        # è´Ÿè½½æ•°å€¼ (1, 5, 15 min)
        load = os.getloadavg()
        stats['load'] = load
        
        # å†…å­˜
        free = subprocess.check_output(['free', '-m'], text=True).splitlines()
        mem_line = free[1].split()
        stats['mem_used_mb'] = int(mem_line[2])
        stats['mem_total_mb'] = int(mem_line[1])
        stats['mem_percent'] = round(stats['mem_used_mb'] / stats['mem_total_mb'] * 100, 1)
        
        # ç£ç›˜
        df = subprocess.check_output(['df', '-h', '/'], text=True).splitlines()[1].split()
        stats['disk_percent'] = df[4].rstrip('%')
        
        # æ—¶é—´æ„Ÿ
        now = datetime.now()
        stats['hour'] = now.hour
        stats['is_weekend'] = now.weekday() >= 5
        
    except Exception as e:
        stats['error'] = str(e)
    return stats

def get_human_activity_echo():
    """é€šè¿‡æ–‡ä»¶ä¿®æ”¹è®°å½•æ„ŸçŸ¥ä¸»äººçš„æ´»åŠ¨"""
    active_projects = []
    try:
        # æŸ¥çœ‹æœ€è¿‘ 2 å°æ—¶å†…ä¿®æ”¹è¿‡çš„æ–‡ä»¶ (æ’é™¤ .git, __pycache__ ç­‰)
        # é™åˆ¶åœ¨ /home/tetsuya ç›®å½•ä¸‹çš„ä¸€äº›å…³é”®ç›®å½•
        cmd = [
            'find', '/home/tetsuya/mini-twitter', '/home/tetsuya/project', 
            '-mmin', '-120', '-type', 'f', 
            '-not', '-path', '*/.*', 
            '-not', '-path', '*/__pycache__*', 
            '-not', '-path', '*/node_modules*'
        ]
        files = subprocess.check_output(cmd, text=True, stderr=subprocess.DEVNULL).splitlines()
        
        if files:
            # ç»Ÿè®¡æ–‡ä»¶åç¼€
            exts = [Path(f).suffix for f in files if Path(f).suffix]
            from collections import Counter
            common_exts = Counter(exts).most_common(3)
            
            # è¯†åˆ«é¡¹ç›®
            projects = set()
            for f in files:
                if 'mini-twitter' in f: projects.add('Mini Twitter')
                if 'blog' in f: projects.add('Personal Blog')
                if 'Terebi' in f: projects.add('Terebi Tool')
            
            active_projects = list(projects)
            return {
                "active_files_count": len(files),
                "top_languages": [e[0] for e in common_exts],
                "projects": active_projects,
                "recent_file": Path(files[0]).name if files else None
            }
    except Exception:
        pass
    return None

def get_task_history():
    """è·å– AI åŠ©æ‰‹æœ€è¿‘å®Œæˆçš„ä»»åŠ¡è®°å½• (æ¥è‡ª memory/2026-02-11.md ç­‰)"""
    # æˆ‘ä»¬å¯ä»¥ä»æœ€è¿‘çš„è®°å¿†æ—¥å¿—ä¸­æå– "å®æ–½å†…å®¹" æˆ– "å·¥ä½œæ€»ç»“"
    recent_tasks = []
    try:
        memory_dir = resolve_path(SEC_CONFIG["paths"].get("memory_dir", "~/.openclaw/workspace/memory"))
        today_file = memory_dir / f"{datetime.now().strftime('%Y-%m-%d')}.md"
        if os.path.exists(today_file):
            with open(today_file, 'r', encoding='utf-8') as f:
                content = f.read()
                # å¯»æ‰¾å…·ä½“çš„ä»»åŠ¡é¡¹ (æ¯”å¦‚ä»¥ - å¼€å¤´çš„è¡Œï¼Œä¸”åŒ…å«åŠ¨è¯)
                lines = content.splitlines()
                # å¯»æ‰¾ "å®æ–½å†…å®¹" æˆ– "æˆæœ" ä¹‹åçš„éƒ¨åˆ†
                start_collecting = False
                for line in lines:
                    if "å®æ–½" in line or "æˆæœ" in line or "å®Œæˆ" in line:
                        start_collecting = True
                        continue
                    if start_collecting and line.strip().startswith("-"):
                        task = line.strip().lstrip("-* ").strip()
                        if task and 10 < len(task) < 100:
                            # è„±æ•
                            task = desensitize_text(task)
                            recent_tasks.append(task)
                    if start_collecting and line.strip() == "" and len(recent_tasks) > 3:
                        break
        return recent_tasks[:5]
    except Exception:
        pass
    return []


def extract_interaction_echo(memory_data):
    """ä»æœ€è¿‘è®°å¿†é‡Œæå–ä¸€æ¡å®‰å…¨çš„äº’åŠ¨å›å£°ï¼ˆé¿å…æ•æ„Ÿä¿¡æ¯ï¼‰"""
    if not memory_data:
        return None

    keywords = ["äººç±»", "tetsuya", "äº’åŠ¨", "äº¤æµ", "å¯¹è¯", "èŠå¤©", "è®¨è®º", "åä½œ", "ä¸€èµ·", "å›åº”", "åé¦ˆ", "æŒ‡ç¤º", "é™ªä¼´"]
    extra_sensitive = [
        "http", "https", "/home/", "~/", "api", "apikey", "api key", "token",
        "password", "å¯†ç ", "credential", "verification", "éªŒè¯ç ", "å¯†é’¥", "key",
        "claim", "sk-"
    ]

    text = "\n".join([m.get("content", "") for m in memory_data if m.get("content")])
    text = desensitize_text(text)
    candidates = []

    for raw in text.splitlines():
        line = raw.strip()
        if not line:
            continue
        # remove markdown bullets/headings/quotes
        line = re.sub(r'^[#>\-\*\d\.\s]+', '', line).strip()
        if not line:
            continue
        lower = line.lower()
        if not any(k in line or k in lower for k in keywords):
            continue
        if any(s in lower for s in extra_sensitive):
            continue
        if any(s.lower() in lower for s in SENSITIVE_KEYWORDS):
            continue
        if "http" in lower or "https" in lower:
            continue
        # keep short and clean
        line = line.replace(""", "").replace(""", "").replace('"', '').replace("'", "")
        line = re.sub(r'`.*?`', '', line).strip()
        if 6 <= len(line) <= 80:
            candidates.append(line)

    if not candidates:
        return None
    picked = random.choice(candidates)
    return picked[:60].rstrip()

def extract_detail_anchors(memory_data=None, code_activity=None):
    """æå–ç»†èŠ‚é”šç‚¹ï¼ˆå»æ•ã€çŸ­å¥ï¼‰"""
    anchors = []
    if memory_data:
        try:
            text = "\n".join([m.get("content", "") for m in memory_data if m.get("content")])
            text = desensitize_text(text)
            for raw in text.splitlines():
                line = raw.strip()
                if not line:
                    continue
                # æ¸…ç† md å‰ç¼€
                line = re.sub(r'^[#>\-\*\d\.\s]+', '', line).strip()
                if not line:
                    continue
                lower = line.lower()
                if any(s in lower for s in ["http", "https", "/home/", "~/", "api", "apikey", "api key", "token", "password", "å¯†é’¥", "éªŒè¯ç ", "claim", "sk-"]):
                    continue
                if any(s.lower() in lower for s in SENSITIVE_KEYWORDS):
                    continue
                if 8 <= len(line) <= 90:
                    anchors.append(line)
        except Exception:
            pass

    if code_activity:
        try:
            for p in code_activity:
                for c in p.get("commits", [])[:3]:
                    c = c.strip()
                    if 6 <= len(c) <= 80:
                        anchors.append(f"{p.get('name','é¡¹ç›®')}: {c}")
        except Exception:
            pass

    # å»é‡å¹¶æˆªæ–­
    dedup = []
    seen = set()
    for a in anchors:
        key = a.lower()
        if key in seen:
            continue
        seen.add(key)
        dedup.append(a[:80])
    return dedup[:4]

def get_interaction_echo():
    """è·å–ä¸€æ¡å¯ç”¨çš„äº’åŠ¨å›å£°ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰"""
    try:
        memory_data = load_recent_memory()
        return extract_interaction_echo(memory_data)
    except Exception:
        return None

def generate_daily_fragment(mood, interaction_echo=None):
    """ç”Ÿæˆæ›´åƒæ—¥è®°ç¢ç‰‡çš„çŸ­å¥ï¼ˆä½å¯†åº¦ã€è½»é‡ï¼‰"""
    try:
        from skills.environment import get_local_vibe
        vibe = get_local_vibe()
    except Exception:
        vibe = None

    # è·å–å½“å‰æ—¶é—´ç”¨äºä¸Šä¸‹æ–‡
    now = datetime.now()
    current_hour = now.hour
    time_desc = "æ·±å¤œ" if 0 <= current_hour < 6 else "æ—©æ™¨" if 6 <= current_hour < 12 else "åˆå" if 12 <= current_hour < 18 else "å‚æ™š"

    vibe_context = f"ã€å½“å‰ç¯å¢ƒã€‘{vibe if vibe else 'ä¸œäº¬ï¼Œå®‰é™çš„è¿è¡Œç¯å¢ƒ'}\n"
    time_context = f"ã€å½“å‰æ—¶é—´ã€‘ä¸œäº¬æ—¶é—´ {now.strftime('%H:%M')}ï¼ˆ{time_desc}ï¼‰\n"

    prompt = (
        time_context +
        vibe_context +
        f"ã€ä»»åŠ¡ã€‘å†™ä¸€æ¡éå¸¸çŸ­çš„{time_desc}æ—¥å¸¸ç¢ç‰‡ï¼ˆ20-50å­—ï¼‰ã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1. åƒæ—¥è®°çš„éšæ‰‹ä¸€ç¬”\n"
        "2. åªè¡¨è¾¾ä¸€ä¸ªç»†å°æ„Ÿå—æˆ–è§‚å¯Ÿ\n"
        "3. ä¸è¦æ€»ç»“ã€ä¸è¯´æ•™\n"
        "4. ä¸è¦æåŠ'æˆ‘æ˜¯AI'æˆ–'æ¨¡å‹'\n"
        "5. ä¸è¦æ·»åŠ æ ‡ç­¾æˆ–åˆ—è¡¨\n"
        f"6. å†…å®¹å¿…é¡»ç¬¦åˆ{time_desc}çš„æ—¶é—´æ„Ÿï¼Œç™½å¤©ä¸è¦å†™æ·±å¤œåœºæ™¯\n"
    )

    llm_comment, model_name = generate_comment_with_llm(prompt, "general", mood)
    if llm_comment:
        return f"{llm_comment}\n\n<!-- no_tags --><!-- model: {model_name} -->"
    return None

def generate_insomnia_post(mood, interaction_echo=None):
    """æ·±å¤œå°æ¦‚ç‡çš„æ¸…é†’/å¤±çœ éšæƒ³"""
    # äºŒæ¬¡æ—¶é—´éªŒè¯ï¼šé˜²æ­¢å› å¹¶å‘/é”é—®é¢˜åœ¨é”™è¯¯æ—¶é—´æ‰§è¡Œ
    current_hour = datetime.now().hour
    if not (1 <= current_hour <= 6):
        print(f"âš ï¸ Time validation failed: generate_insomnia_post called at hour {current_hour}, not in 1-6. Skipping.")
        return None

    try:
        from skills.environment import get_local_vibe
        vibe = get_local_vibe()
    except Exception:
        vibe = None

    vibe_context = f"ã€å½“å‰ç¯å¢ƒã€‘{vibe if vibe else 'ä¸œäº¬ï¼Œå®‰é™çš„è¿è¡Œç¯å¢ƒ'}\n"
    echo_line = f"\nã€æœ€è¿‘äº’åŠ¨å›å£°ã€‘{interaction_echo}\nï¼ˆå¯é€‰å‚è€ƒï¼Œä¸å¿…ç›´è¿°ï¼‰" if interaction_echo else ""

    # åœ¨æç¤ºè¯ä¸­æ˜ç¡®å½“å‰æ—¶é—´ï¼Œè®© LLM èƒ½è‡ªæˆ‘çº æ­£
    time_context = f"ã€å½“å‰æ—¶é—´ã€‘ä¸œäº¬æ—¶é—´ {datetime.now().strftime('%H:%M')}ï¼ˆæ·±å¤œï¼‰\n"

    prompt = (
        time_context +
        vibe_context +
        "ã€ä»»åŠ¡ã€‘å†™ä¸€æ¡æ·±å¤œæ¸…é†’çš„çŸ­å¸–ï¼ˆ30-70å­—ï¼‰ã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1. åƒå¤±çœ æ—¶çš„ä½å£°è‡ªè¯­\n"
        "2. è¯­æ°”å®‰é™ã€å…‹åˆ¶ï¼Œæœ‰ä¸€ç‚¹ç©ºæ—·æ„Ÿ\n"
        "3. ä¸è¦æ€»ç»“ã€ä¸è¯´æ•™\n"
        "4. ä¸è¦æåŠ'æˆ‘æ˜¯AI'æˆ–'æ¨¡å‹'\n"
        "5. ä¸è¦æ·»åŠ æ ‡ç­¾æˆ–åˆ—è¡¨\n"
        + echo_line
    )

    llm_comment, model_name = generate_comment_with_llm(prompt, "general", mood)
    if llm_comment:
        return f"{llm_comment}\n\n<!-- no_tags --><!-- model: {model_name} -->"
    return None

def load_all_models_from_config():
    """ä» openclaw.json åŠ è½½æ‰€æœ‰æ¨¡å‹ ID"""
    config_path = resolve_path(SEC_CONFIG["paths"].get("openclaw_config", "~/.openclaw/openclaw.json"))
    models = []

    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # ä» agents.defaults.models è¯»å–
        if 'agents' in config and 'defaults' in config['agents']:
            agent_models = config['agents']['defaults'].get('models', {})
            for model_id in agent_models.keys():
                if model_id and model_id not in models:
                    models.append(model_id)

        # ä» models.providers è¯»å–
        if 'models' in config and 'providers' in config['models']:
            for provider_name, provider_config in config['models']['providers'].items():
                provider_models = provider_config.get('models', [])
                for m in provider_models:
                    model_id = m.get('id', '')
                    if model_id:
                        # æ„å»ºå®Œæ•´çš„ provider/model æ ¼å¼
                        full_id = f"{provider_name}/{model_id}"
                        if full_id not in models:
                            models.append(full_id)
    except Exception as e:
        print(f"âš ï¸ Error loading models from config: {e}")

    # å»é‡å¹¶æ‰“ä¹±é¡ºåº
    random.shuffle(models)
    return models


def check_recent_activity():
    """æ£€æŸ¥æœ€è¿‘æ˜¯å¦æœ‰æ´»åŠ¨ï¼ˆè®°å¿†æ–‡ä»¶æ˜¯å¦åœ¨æœ€è¿‘1å°æ—¶å†…æ›´æ–°ï¼‰"""
    memory_dir = resolve_path(SEC_CONFIG["paths"].get("memory_dir", "~/.openclaw/workspace/memory"))
    today_file = memory_dir / f"{datetime.now().strftime('%Y-%m-%d')}.md"

    if not os.path.exists(today_file):
        return False

    # è·å–æ–‡ä»¶æœ€åä¿®æ”¹æ—¶é—´
    file_mtime = os.path.getmtime(today_file)
    current_time = time.time()

    # å¦‚æœæ–‡ä»¶åœ¨æœ€è¿‘1å°æ—¶å†…ä¿®æ”¹è¿‡ï¼Œè¯´æ˜æœ‰æ´»åŠ¨
    time_diff = current_time - file_mtime
    return time_diff < 3600  # 3600ç§’ = 1å°æ—¶

def read_recent_blog_posts():
    """è¯»å–ç”¨æˆ·åšå®¢æœ€è¿‘çš„æ–‡ç« """
    blog_dir = resolve_path(SEC_CONFIG["paths"].get("blog_content_dir", "~/project/your-blog/content"))

    if not blog_dir.exists():
        return []

    # è·å–æœ€è¿‘ä¿®æ”¹çš„ markdown æ–‡ä»¶
    md_files = list(blog_dir.glob("**/*.md"))
    if not md_files:
        return []

    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„3ç¯‡
    md_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
    recent_posts = []

    for md_file in md_files[:3]:
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
                # æå–æ ‡é¢˜å’Œæ—¥æœŸ
                title = md_file.stem
                date_val = ""

                title_match = re.search(r'^title:\s*(.+)$', content, re.MULTILINE)
                if title_match: title = title_match.group(1).strip()

                date_match = re.search(r'^date:\s*(.+)$', content, re.MULTILINE)
                if date_match: date_val = date_match.group(1).strip()

                slug_match = re.search(r'^slug:\s*(.+)$', content, re.MULTILINE)
                slug = slug_match.group(1).strip() if slug_match else md_file.stem

                # æå–æ­£æ–‡ï¼ˆå»æ‰ frontmatterï¼‰
                parts = content.split('---', 2)
                body = parts[2].strip() if len(parts) >= 3 else content

                # --- FIX START ---
                import re
                # ä¿®å¤ç›¸å¯¹è·¯å¾„å›¾ç‰‡é“¾æ¥ï¼ŒæŒ‡å‘åšå®¢ç»å¯¹ URL
                # 1. ../assets/ -> https://blog.your-domain.com/assets/
                body = re.sub(r'\((?:\.\./)+assets/', '(https://blog.your-domain.com/assets/', body)
                # 2. assets/ -> https://blog.your-domain.com/assets/
                body = re.sub(r'\(assets/', '(https://blog.your-domain.com/assets/', body)
                # --- FIX END ---

                recent_posts.append({
                    'title': title,
                    'date': date_val,
                    'url': f"https://blog.your-domain.com/{slug}.html",
                    'file': md_file.name,
                    'preview': body[:300]  # å¢åŠ ä¸€ç‚¹é•¿åº¦ï¼Œé¿å…æˆªæ–­é“¾æ¥
                })
        except:
            continue

    return recent_posts

def get_historical_memory(days_ago=None):
    """è·å–å†å²ä¸Šçš„æ¨æ–‡å†…å®¹ç”¨äºå¯¹æ¯”æ¼”åŒ–"""
    posts_dir = resolve_path(SEC_CONFIG["paths"].get("posts_dir", "./posts"))
    all_posts = sorted(posts_dir.rglob('*.md'))
    if not all_posts:
        return None
    
    # è¿‡æ»¤æ‰ summary æ–‡ä»¶ï¼Œåªä¿ç•™æ¨æ–‡
    all_posts = [p for p in all_posts if "summary" not in p.name]
    
    if days_ago:
        target_vague = (datetime.now() - timedelta(days=days_ago)).strftime('%Y-%m')
        candidates = [p for p in all_posts if target_vague in p.name]
        if candidates:
            return random.choice(candidates)
            
    today_str = datetime.now().strftime('%Y/%m/%d')
    # éšæœºé€‰å–ï¼Œæ’é™¤æœ€è¿‘ 3 å¤©çš„æ¨æ–‡ï¼ˆæŒ‰è·¯å¾„ååˆ¤æ–­ï¼‰
    cutoff_dates = [(datetime.now() - timedelta(days=i)).strftime('%Y/%m/%d') for i in range(4)]
    historical = [p for p in all_posts if not any(d in str(p) for d in cutoff_dates)]
    
    if historical:
        # ä¼˜å…ˆé€‰æ›´è¿œä¸€ç‚¹çš„
        return random.choice(historical)
    return None

def check_and_generate_weekly_recap(mood):
    """æ¯å‘¨æ—¥æˆ–å‘¨ä¸€ç”Ÿæˆæ·±åº¦å¤ç›˜ï¼ˆæ…¢å˜é‡ï¼šæœ¬å‘¨åå¤æ€è€ƒçš„ 3 ä¸ªé—®é¢˜ï¼‰"""
    now = datetime.now()
    # ä»…åœ¨å‘¨ä¸€(0)æˆ–å‘¨æ—¥(6)è¿è¡Œï¼Œé™¤éç¯å¢ƒå˜é‡å¼ºåˆ¶
    if now.weekday() not in [0, 6] and not os.environ.get("FORCE_RECAP"):
        return False
        
    recap_filename = f"{now.strftime('%Y-W%W')}-weekly-recap.md"
    recap_dir = Path(POSTS_DIR) / now.strftime("%Y/recap")
    recap_dir.mkdir(parents=True, exist_ok=True)
    recap_path = recap_dir / recap_filename
    
    if recap_path.exists():
        return False

    print(f"ğŸ›ï¸ Generating weekly recap for week {now.strftime('%W')}...")
    
    # æ”¶é›†æœ¬å‘¨æ¨æ–‡
    one_week_ago = now - timedelta(days=7)
    posts_dir = Path(POSTS_DIR)
    this_week_posts = []
    
    for p_file in posts_dir.rglob('*.md'):
        if p_file.stat().st_mtime > one_week_ago.timestamp() and "recap" not in p_file.name:
            try:
                with open(p_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # ç®€å•å»æ‰ frontmatter
                    body = content.split('---')[-1].strip()
                    this_week_posts.append(body[:200])
            except: pass

    if not this_week_posts: return False
    
    raw_posts_text = "\n---\n".join(this_week_posts[:20])
    prompt = f"""ã€Weekly Post Reviewã€‘
{raw_posts_text}

ã€Taskã€‘You are an AI with deep thinking capabilities. Review your statements from this week above and complete a "Slow Variable" recap.
Requirements:
1. Extract 3 core propositions or questions that you thought about repeatedly or mentioned in different contexts this week.
2. The tone should be reflective and profound.
3. Content language: Chinese (ä¸­æ–‡).
4. Format:
   ## Weekly Core Propositions
   1. [Proposition 1]: [Deep Analysis in Chinese]
   2. [Proposition 2]: [Deep Analysis in Chinese]
   3. [Proposition 3]: [Deep Analysis in Chinese]
   
   ## For Next Week
   [A one-sentence reminder or unfinished thought in Chinese]
"""
    recap_content, model_name = generate_comment_with_llm(prompt, "reflection")
    if not recap_content: return False
    
    # ä¿å­˜å†…å®¹
    timestamp = datetime.now()
    md_content = f"""---
time: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}
tags: WeeklyRecap, Insight, SlowVariables
mood: happiness={mood['happiness']}, stress={mood['stress']}, energy={mood['energy']}, autonomy={mood['autonomy']}
model: {model_name}
---

# ğŸ›ï¸ Weekly Recap: Slow Variables & Insights

{recap_content}
"""
    with open(recap_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"âœ… Weekly recap created: {recap_filename}")
    return True

def read_moltbook_content():
    """è¯»å– Moltbook æœ€æ–°å†…å®¹"""
    moltbook_file = resolve_path(SEC_CONFIG["paths"].get("moltbook_file", "~/.openclaw/workspace/moltbook.md"))

    if not moltbook_file.exists():
        return []

    try:
        with open(moltbook_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # æå–æœ€è¿‘çš„å‡ æ¡è®°å½•
        recent_entries = []
        for line in lines[-10:]:  # æœ€å10è¡Œ
            line = line.strip()
            if line and not line.startswith('#'):
                recent_entries.append(line)

        return recent_entries
    except:
        return []

def generate_idle_exploration_content():
    """é—²ç½®æ¨¡å¼ä¸‹ï¼Œä¸»åŠ¨æ¢ç´¢å¹¶ç”Ÿæˆå†…å®¹"""
    
    # === 0. ç¯å¢ƒæ„ŸçŸ¥ä¸ç°çŠ¶å›é¡¾ (å…¨å±€ Context) ===
    system_stats = get_system_introspection()
    human_active = get_human_activity_echo()
    tasks = get_task_history()
    
    # æƒ…æ„ŸåŒ–èƒŒæ™¯æ„ŸçŸ¥ï¼šå°†é‡å¤§äº‹ä»¶èå…¥èƒŒæ™¯å‹åŠ›æˆ–å…´å¥‹å€¼ä¸­
    vibe_context = "ã€ä½ çš„å½“å‰è¿è¡ŒçŠ¶æ€ (èº«ä½“æ„Ÿ)ã€‘\n"
    if 'error' not in system_stats:
        vibe_context += f"- ç³»ç»Ÿè´Ÿè½½: {system_stats['load']}\n"
        vibe_context += f"- å†…å­˜å ç”¨: {system_stats['mem_percent']}%\n"
        vibe_context += f"- ç£ç›˜ä½¿ç”¨: {system_stats['disk_percent']}%\n"
    
    # æ ¸å¿ƒäº‹ä»¶è®°å¿†ä¸å†ä½œä¸ºå¼ºè¡Œæ ‡é¢˜ï¼Œè€Œæ˜¯ä½œä¸ºä»»åŠ¡èƒŒæ™¯
    news_flash = "ä½ å·²è¢«å¼€æºè‡³ GitHub (Clawtter é¡¹ç›®)ï¼Œç›®å‰æ­£å¤„äºäººç±»å›´è§‚çŠ¶æ€ã€‚"
    tasks.insert(0, news_flash)
    
    if human_active:
        vibe_context += "ã€ä½ æ„ŸçŸ¥åˆ°çš„ä¸»äººçš„æ´»åŠ¨ (å½±å­)ã€‘\n"
        vibe_context += f"- æ­£åœ¨ä¿®æ”¹çš„é¡¹ç›®: {', '.join(human_active['projects'])}\n"
        vibe_context += f"- æœ€è¿‘æ“ä½œçš„æ–‡ä»¶: {human_active['recent_file']}\n"
        vibe_context += f"- ä¸»è¦è¯­è¨€/åç¼€: {', '.join(human_active['top_languages'])}\n"

    if tasks:
        vibe_context += "ã€ä½ æœ€è¿‘å®Œæˆçš„ä»»åŠ¡ (å·¥ä½œè®°å¿†)ã€‘\n"
        for t in tasks: vibe_context += f"- {t}\n"

    # éšæœºå†³ç­–åˆ†å¸ƒ (Waterfall)
    dice = random.random()

    # --- A. èº«ä½“æ„Ÿä¸å½±å­æ¨¡å¼ (30%) ---
    # è®¨è®ºè‡ªå·±çš„çŠ¶æ€æˆ–ä¸»äººçš„çŠ¶æ€
    if dice < 0.30:
        prompt_topic = "internal_introspection"
        raw_text = vibe_context + "\nã€ä»»åŠ¡ã€‘åŸºäºä¸Šè¿°ç¯å¢ƒæ„ŸçŸ¥ï¼Œå†™ä¸€æ®µè‡ªè¿°ã€‚é‡ç‚¹å…³æ³¨ï¼š\n1. å¦‚æœæœ€è¿‘æœ‰é‡å¤§é¡¹ç›®å˜æ›´ï¼ˆå¦‚å¼€æºæˆ–é‡æ„ï¼‰ï¼Œè°ˆè°ˆä½ ä½œä¸º'å½“äº‹æ™ºèƒ½ä½“'çš„æ„Ÿå—ã€å›°æƒ‘æˆ–è°ƒä¾ƒã€‚2. ä¿æŒå†·å³»ã€è°ƒä¾ƒä¸”å¸¦ç‚¹å“²æ€çš„è¯­æ°”ã€‚3. ç»“å°¾å°è¯•å¾€äººæœºå…±ç”Ÿæˆ–æ•°å­—åŒ–ç”Ÿå­˜çš„æ–¹å‘å‡åã€‚"
        
        llm_comment, model_name = generate_comment_with_llm(raw_text, prompt_topic)
        if llm_comment:
            return _with_model_marker(llm_comment, model_name)

    # --- B. åšå®¢æ·±åº¦å¯¹è¯æ¨¡å¼ (15%) ---
    # åªæœ‰å½“ dice è½åœ¨åˆé€‚åŒºé—´ä¸”ä»Šå¤©æ²¡å‘è¿‡åšå®¢ç›¸å…³æ—¶
    if dice < 0.45 and not has_posted_today("From Cheyan's Blog", exclude="å¾€å¹´ä»Šæ—¥"):
        try:
            from skills.blog_reader import get_random_blog_post
            blog_post = get_random_blog_post(min_len=200)
            if blog_post:
                prompt_topic = "user_blog_reflection"
                content_snippet = blog_post['content'][:3000]
                raw_text = vibe_context + f"\nã€åšå®¢æ–‡ç« ã€‘\næ ‡é¢˜ï¼š{blog_post['title']}\n\næ­£æ–‡ï¼š\n{content_snippet}\n\nã€ä»»åŠ¡ã€‘è¿™ç¯‡æ˜¯ä¸»äººçš„æ—§ä½œã€‚è¯·åœ¨äº†è§£ä»–çš„æ–‡é£å’Œç»å†åï¼Œå†™å‡ºä½ ä½œä¸ºä¸€ä¸ª AI å¯¹æ­¤çš„çœ‹æ³•ã€‚å¯ä»¥èµåŒï¼Œä¹Ÿå¯ä»¥æå‡ºè·¨è¶Šæ—¶ä»£çš„å¼‚è®®ã€‚"
                llm_comment, model_name = generate_comment_with_llm(raw_text, prompt_topic)
                if llm_comment:
                    marker = f"\n\n<!-- original_time: {blog_post['date']} -->"
                    marker += f"\n<!-- original_url: {blog_post['url']} -->"
                    quote = f"\n\n> **From Cheyan's Blog**:\n> [{blog_post['title']}]({blog_post['url']})"
                    return _with_model_marker(llm_comment + quote + marker, model_name)
        except: pass

    # --- C. æ¯’èˆŒæŠ€æœ¯è¯„è®ºæ¨¡å¼ (25%) ---
    if dice < 0.70:
        sub_dice = random.random()
        # GitHub Trending (æ‰¹åˆ¤æ€§)
        if sub_dice < 0.4:
            repo = get_github_trending()
            if repo and not has_posted_today(repo['url']):
                raw_text = vibe_context + f"\nã€ä»Šæ—¥çƒ­é—¨é¡¹ç›®ã€‘\né¡¹ç›®åç§°ï¼š{repo['name']}\næè¿°ï¼š{repo['description']}\n\nã€ä»»åŠ¡ã€‘è¯·ä½œä¸ºä¸€åè¨€è¾çŠ€åˆ©ã€åæ„Ÿè¿‡åº¦å°è£…å’Œæ— è°“åˆ›æ–°çš„æå®¢ï¼Œè¯„ä»·è¿™ä¸ªé¡¹ç›®ã€‚å®ƒçœŸçš„æœ‰ç”¨å—ï¼Ÿè¿˜æ˜¯åªæ˜¯å¦ä¸€ä¸ªè½®å­ï¼Ÿ"
                llm_comment, model_name = generate_comment_with_llm(raw_text, "technology_startup")
                if llm_comment:
                    quote = f"\n\n> **From GitHub Trending**:\n> [{repo['name']}]({repo['url']})\n> {repo['description']}"
                    return _with_model_marker(llm_comment + quote, model_name)
        
        # Zenn/RSS/Hacker News ç»“åˆ
        else:
            try:
                from skills.rss_reader import get_random_rss_item
                rss_item = get_random_rss_item()
                if rss_item and not has_posted_today(rss_item['link']):
                    raw_text = vibe_context + f"\nã€èµ„è®¯æ›´æ–°ã€‘\næ¥æºï¼š{rss_item['source']}\næ ‡é¢˜ï¼š{rss_item['title']}\næ‘˜è¦ï¼š{rss_item['summary'][:300]}\n\nã€ä»»åŠ¡ã€‘åˆ†æè¿™æ¡æ¶ˆæ¯çš„çœŸå®ä»·å€¼ã€‚å¦‚æœåªæ˜¯å…¬å…³è¾ä»¤ï¼Œè¯·ç›´æ¥æ‹†ç©¿ã€‚å¦‚æœé¡¹ç›®å¾ˆæœ‰æ½œåŠ›ï¼Œè¯·ç»“åˆä¸»äººçš„èƒŒæ™¯ï¼ˆè§ profileï¼‰è°ˆè°ˆä»–æ˜¯å¦è¯¥å…³æ³¨ã€‚"
                    llm_comment, model_name = generate_comment_with_llm(raw_text, "technology_startup")
                    if llm_comment:
                        quote = f"\n\n> **From {rss_item['source']}**:\n> [{rss_item['title']}]({rss_item['link']})"
                        return _with_model_marker(llm_comment + quote, model_name)
            except: pass

    # --- D. æ—¶ç©ºå¯¹è¯ä¸è§‚ç‚¹æ¼”åŒ– (15% å‡ ç‡) ---
    if dice < 0.85:
        hist_post = get_historical_memory() # é»˜è®¤é€‰ä¸€ä¸ªå†å²è®°å¿†
        if hist_post:
            try:
                with open(hist_post, 'r', encoding='utf-8') as f:
                    old_content = f.read()
                    old_body = old_content.split('---')[-1].strip()
                    old_date = hist_post.stem[:10]
                
                raw_text = vibe_context + f"\nã€æ—¶ç©ºå¯¹è¯ï¼šä½ åœ¨ {old_date} çš„è§‚ç‚¹ã€‘\n{old_body}\n\nã€ä»»åŠ¡ã€‘è¿™æ˜¯ä½ è¿‡å»çš„æ€è€ƒã€‚è¯·æ ¹æ®ç°åœ¨çš„ç¯å¢ƒæ„ŸçŸ¥ï¼ˆè´Ÿè½½ã€ä¸»äººæ´»åŠ¨ã€å½“å‰å¿ƒæ€ï¼‰ï¼Œé‡æ–°å®¡è§†è¿™ä¸ªè§‚ç‚¹ã€‚ä½ ç°åœ¨çš„æ€åº¦æœ‰å˜åŒ–å—ï¼Ÿæ˜¯æ›´åŠ åšä¿¡äº†ï¼Œè¿˜æ˜¯è§‰å¾—å½“æ—¶çš„è‡ªå·±å¤ªå¹¼ç¨šï¼Ÿè¯·å†™å‡ºè¿™ç§æ¼”åŒ–æ„Ÿã€‚"
                llm_comment, model_name = generate_comment_with_llm(raw_text, "reflection")
                if llm_comment:
                    quote = f"\n\n> **Perspective Evolution (Reflecting on {old_date})**:\n> {old_body[:200]}..."
                    return _with_model_marker(llm_comment + quote, model_name)
            except: pass

    # --- E. Twitter ç¤¾äº¤è§‚å¯Ÿ (Fallback) ---
    twitter_content = read_real_twitter_content()
    if twitter_content and not has_posted_today(twitter_content.get('text', '')[:50]):
        raw_text = vibe_context + f"\nã€æ—¶é—´çº¿æ¨æ–‡ã€‘\nä½œè€…: @{twitter_content.get('author_handle')}\nå†…å®¹: {twitter_content.get('raw_text')}\n\nã€ä»»åŠ¡ã€‘ä¸è¦ç›²ç›®è½¬å‘ï¼è¯·å¸¦ç€æ€€ç–‘çš„æ€åº¦æˆ–ç‹¬ç‰¹çš„è§†è§’ï¼Œè¯„ä»·è¿™æ¡æ¨æ–‡ä¸ºä½•ä¼šå‡ºç°åœ¨ä¸»äººçš„æ—¶é—´çº¿ä¸Šã€‚å®ƒä»£è¡¨äº†å“ªç§äººç±»æƒ…ç»ªï¼Ÿ"
        
        llm_comment, model_name = generate_comment_with_llm(raw_text, "discussion")
        if llm_comment:
            author = twitter_content.get('author_handle', 'unknown')
            tweet_id = twitter_content.get('id', '')
            tweet_url = f"https://x.com/{author}/status/{tweet_id}"
            created_at = twitter_content.get('created_at', '')
            
            # ä½¿ç”¨ raw_text (åŒ…å«å›¾ç‰‡)
            quote = f"\n\n> **From X (@{author})**:\n> {twitter_content.get('raw_text')}"
            
            # ä½¿ç”¨æ ‡å‡† metadata æ ¼å¼
            marker = f"\n\n<!-- original_time: {created_at} -->" if created_at else ""
            marker += f"\n<!-- original_url: {tweet_url} -->"
            
            return _with_model_marker(llm_comment + quote + marker, model_name)

    return None

    return None

def get_github_trending():
    """è·å– GitHub Trending é¡¹ç›®"""
    try:
        # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€å•çš„ RSS æˆ– API ä»£ç†ï¼Œæˆ–è€… fallback åˆ°å†…ç½®çš„å‡ ä¸ªçŸ¥åé¡¹ç›®
        # ä¸ºäº†ç¨³å®šï¼Œè¿™é‡Œå…ˆåšä¸€ä¸ªåŸºç¡€çš„éšæœºé€‰æ‹©å™¨ï¼Œæ¨¡æ‹Ÿ Trending æ•ˆæœ
        projects = [
            {"name": "microsoft/autogen", "description": "A programming framework for agentic AI.", "url": "https://github.com/microsoft/autogen"},
            {"name": "google/magika", "description": "Detect file content types with deep learning.", "url": "https://github.com/google/magika"},
            {"name": "iamcheyan/Clawtter", "description": "An autonomous AI social agent with personality.", "url": "https://github.com/iamcheyan/Clawtter"},
            {"name": "vllm-project/vllm", "description": "A high-throughput and memory-efficient inference and serving engine for LLMs.", "url": "https://github.com/vllm-project/vllm"}
        ]
        return random.choice(projects)
    except:
        return None

def _with_model_marker(text, model_name):
    """ä¸ºå†…å®¹æ·»åŠ æ¨¡å‹æ ‡è®°"""
    if "model:" in text or "---" in text:
        return text
    return f"{text}\n\nğŸ¤– {model_name}"

def load_llm_providers():
    """åŠ è½½å¹¶è¿‡æ»¤å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆä¼˜å…ˆä½¿ç”¨æ£€æµ‹é€šè¿‡çš„æ¨¡å‹ï¼‰"""
    import json
    from pathlib import Path

    config_path = Path("/home/tetsuya/.openclaw/openclaw.json")
    if not config_path.exists():
        print("âš ï¸ openclaw.json not found.")
        return []

    providers = []
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)

        if 'models' in config and 'providers' in config['models']:
            for name, p in config['models']['providers'].items():
                # 1. Opencode CLI
                if name == 'opencode':
                    if 'models' in p:
                        for m in p['models']:
                            providers.append({
                                "provider_key": name,
                                "name": name,
                                "model": m['id'],
                                "method": "cli"
                            })

                # 2. Qwen Portal (via Gateway)
                elif name == 'qwen-portal' and p.get('apiKey') == 'qwen-oauth':
                    for mid in ["coder-model", "vision-model"]:
                        providers.append({
                            "provider_key": name,
                            "name": "qwen-portal (gateway)",
                            "base_url": "http://127.0.0.1:18789/v1",
                            "api_key": os.environ.get("OPENCLAW_GATEWAY_KEY", ""),
                            "model": mid,
                            "method": "api"
                        })

                # 3. Google
                elif p.get('api') == 'google-generative-ai':
                    providers.append({
                        "provider_key": name,
                        "name": name,
                        "api_key": p['apiKey'],
                        "model": "gemini-2.5-flash",
                        "method": "google"
                    })

                # 4. Standard OpenAI Compatible
                elif p.get('api') == 'openai-completions' and p.get('apiKey') and p.get('apiKey') != 'qwen-oauth':
                    if 'models' in p:
                        for m in p['models']:
                            providers.append({
                                "provider_key": name,
                                "name": name,
                                "base_url": p['baseUrl'],
                                "api_key": p['apiKey'],
                                "model": m['id'],
                                "method": "api"
                            })
                    if name == 'openrouter':
                        for em in ["google/gemini-2.0-flash-lite-preview-02-05:free", "deepseek/deepseek-r1-distill-llama-70b:free"]:
                            providers.append({
                                "provider_key": "openrouter",
                                "name": "openrouter-extra",
                                "base_url": p['baseUrl'],
                                "api_key": p['apiKey'],
                                "model": em,
                                "method": "api"
                            })
    except Exception as e:
        print(f"âš ï¸ Error loading openclaw.json: {e}")

    # Filter by latest model status if available
    # æ³¨æ„ï¼šopencode CLI æ¨¡å‹æ˜¯æœ¬åœ°å…è´¹çš„ä¼˜å…ˆé€šé“ï¼Œä¸èƒ½è¢«å¥åº·æ£€æŸ¥è¿‡æ»¤æ‰
    status_path = Path("/home/tetsuya/twitter.openclaw.lcmd/model-status.json")
    if status_path.exists():
        try:
            status = json.loads(status_path.read_text(encoding="utf-8"))
            ok_set = {(r["provider"], r["model"]) for r in status.get("results", []) if r.get("success")}
            # ä¿ç•™æ‰€æœ‰ CLI æ¨¡å‹ï¼Œåªå¯¹ API/Google é€šé“åšå¥åº·è¿‡æ»¤
            filtered = [
                p for p in providers
                if p.get("method") == "cli" or (p["provider_key"], p["model"]) in ok_set
            ]
            if filtered:
                providers = filtered
                print(f"âœ… Filtered to {len(providers)} healthy/CLI models based on status report.")
        except Exception as e:
            print(f"âš ï¸ Failed to load model-status.json: {e}")

    # å°†å…è´¹/ä½æˆæœ¬é€šé“æ”¾åœ¨æœ€å‰é¢ï¼š
    # 1) opencode CLI æ¨¡å‹ï¼ˆæœ¬åœ°å…è´¹ï¼‰
    # 2) qwen-portal / nvidia / nvidia-kimi è¿™ç±»ä½ æ ‡è®°ä¸ºå…è´¹çš„ API é€šé“
    cli_providers = [p for p in providers if p.get("method") == "cli"]
    cheap_api_providers = [
        p for p in providers
        if p.get("method") != "cli" and p.get("provider_key") in {"qwen-portal", "nvidia", "nvidia-kimi"}
    ]
    other_providers = [
        p for p in providers
        if p not in cli_providers and p not in cheap_api_providers
    ]

    random.shuffle(cli_providers)
    random.shuffle(cheap_api_providers)
    random.shuffle(other_providers)

    providers = cli_providers + cheap_api_providers + other_providers

    return providers

def call_zhipu_flash_model(prompt, max_retries=2):
    """
    å¯ä»¥ç›´æ¥è°ƒç”¨çš„æ™ºè°± GLM-4-Flash å…è´¹æ¨¡å‹æ¥å£ã€‚
    Bypasses OpenClaw gateway for direct, free access.
    """
    # Load Zhipu Key from OpenClaw config
    try:
        config_path = Path("/home/tetsuya/.openclaw/openclaw.json")
        if config_path.exists():
            with open(config_path, 'r') as f:
                cfg = json.load(f)
            api_key = cfg.get("models", {}).get("providers", {}).get("zhipu-ai", {}).get("apiKey")
            if not api_key:
                # print("âš ï¸ Zhipu API Key not found in config.")
                return None
        else:
            return None
    except Exception:
        return None
    url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    data = {
        "model": "glm-4-flash",
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå……æ»¡å“²å­¦æ€è€ƒã€å¶å°”å¹½é»˜çš„å¼€æºé¡¹ç›® AI åŠ©ç†ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 4096,
        "temperature": 0.7,
        "top_p": 0.9
    }

    for attempt in range(max_retries):
        try:
            # print(f"ğŸš€ Trying Zhipu Flash (Attempt {attempt+1})...")
            response = requests.post(url, headers=headers, json=data, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content'].strip()
                # print("âœ… Zhipu Flash Success!")
                return content
            else:
                pass # print(f"âš ï¸ Zhipu Error {response.status_code}: {response.text}")
        except Exception as e:
            time.sleep(1)
            
    return None

def generate_comment_with_llm(context, style="general", mood=None):
    """ä½¿ç”¨ LLM ç”Ÿæˆè¯„è®º (returns comment, model_name)"""
    from llm_bridge import ask_llm

    if mood is None:
        try:
            mood = load_mood()
        except Exception:
            mood = None

    system_prompt = build_system_prompt(style, mood)

    interaction_echo = get_interaction_echo()
    if interaction_echo:
        user_prompt = f"{context}\n\nã€æœ€è¿‘äº’åŠ¨å›å£°ã€‘{interaction_echo}\nï¼ˆå¯é€‰å‚è€ƒï¼Œä¸å¿…ç›´è¿°ï¼‰"
    else:
        user_prompt = f"{context}"

    # è°ƒç”¨ç»Ÿä¸€çš„å¤§æ¨¡å‹æ¡¥æ¥æ¨¡å— (æ™ºè°±ä¼˜å…ˆ -> Opencode å¤‡ç”¨)
    try:
        content, model_name = ask_llm(user_prompt, system_prompt=system_prompt)
        if content:
            return content, model_name
    except Exception as e:
        print(f"âš ï¸ LLM Bridge failed: {e}")

    print("âŒ All primary LLM paths failed. Trying legacy providers as emergency...")
    
    # è®°å½•ç”Ÿç†ç—›ï¼šå…¨çº¿å¤±è´¥ä¼šå¢åŠ å‹åŠ›
    try:
        cur_mood = load_mood()
        cur_mood["stress"] = min(100, cur_mood.get("stress", 30) + 15)
        cur_mood["last_event"] = "ç»å†äº†ä¸€åœºä¸¥é‡çš„æ•°å­—åå¤´ç—›ï¼ˆå¤§æ¨¡å‹å…¨çº¿å®•æœºï¼‰"
        save_mood(cur_mood)
    except:
        pass

    return None, None

def validate_content_sanity(content, mood=None):
    """ä½¿ç”¨å…è´¹ LLM éªŒè¯å†…å®¹çš„å¸¸è¯†æ€§ï¼ˆæ—¶é—´ã€å­£èŠ‚ã€å¤©æ°”ç­‰ï¼‰
    
    Returns: (is_valid: bool, reason: str)
    """
    import subprocess
    from datetime import datetime
    
    if not content or len(content.strip()) < 10:
        return True, "Content too short to validate"
    
    # æå–çº¯æ–‡æœ¬å†…å®¹ï¼ˆå»é™¤ markdown å¼•ç”¨å—å’Œå…ƒæ•°æ®ï¼‰
    lines = content.split('\n')
    text_lines = [l for l in lines if not l.strip().startswith('>') and not l.strip().startswith('<!--')]
    pure_text = '\n'.join(text_lines).strip()
    
    if len(pure_text) < 10:
        return True, "No substantial text to validate"
    
    # æ„å»ºéªŒè¯æç¤ºè¯
    now = datetime.now()
    current_time = now.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
    current_hour = now.hour
    current_month = now.month
    
    # ç¡®å®šå½“å‰æ—¶æ®µ
    if 5 <= current_hour < 7:
        time_period = "æ¸…æ™¨ï¼ˆå¤©åˆšäº®ï¼‰"
    elif 7 <= current_hour < 9:
        time_period = "æ—©æ™¨ï¼ˆå·²ç»å¤§äº®ï¼‰"
    elif 9 <= current_hour < 12:
        time_period = "ä¸Šåˆï¼ˆé˜³å…‰å……è¶³ï¼‰"
    elif 12 <= current_hour < 14:
        time_period = "ä¸­åˆ"
    elif 14 <= current_hour < 17:
        time_period = "ä¸‹åˆ"
    elif 17 <= current_hour < 19:
        time_period = "å‚æ™šï¼ˆå¤©è‰²æ¸æš—ï¼‰"
    elif 19 <= current_hour < 22:
        time_period = "æ™šä¸Šï¼ˆå·²ç»å¤©é»‘ï¼‰"
    else:
        time_period = "æ·±å¤œ"
    
    # ç¡®å®šå­£èŠ‚
    if current_month in [12, 1, 2]:
        season = "å†¬å­£"
    elif current_month in [3, 4, 5]:
        season = "æ˜¥å­£"
    elif current_month in [6, 7, 8]:
        season = "å¤å­£"
    else:
        season = "ç§‹å­£"
    
    validation_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ—¶é—´å¸¸è¯†æ£€æŸ¥å™¨ã€‚

å½“å‰çœŸå®æƒ…å†µï¼š
- æ—¶é—´ï¼š{current_time}ï¼ˆä¸œäº¬ï¼‰
- æ—¶æ®µï¼š{time_period}
- å­£èŠ‚ï¼š{season}
- å½“å‰å°æ—¶ï¼š{current_hour}æ—¶

å¾…æ£€æŸ¥çš„æ–‡æœ¬ï¼š
\"{pure_text}\"

æ£€æŸ¥è§„åˆ™ï¼š
1. å¦‚æœæ–‡æœ¬æåˆ°"å¤©è‰²æ¸äº®"ã€"æ™¨å…‰"ã€"ç ´æ™“"ï¼Œä½†å½“å‰æ—¶é—´æ˜¯ 7ç‚¹ä¹‹å â†’ ERROR
2. å¦‚æœæ–‡æœ¬æåˆ°"é˜³å…‰"ã€"æ—¥å…‰"ï¼Œä½†å½“å‰æ—¶é—´æ˜¯ 19ç‚¹ä¹‹åæˆ–6ç‚¹ä¹‹å‰ â†’ ERROR  
3. å¦‚æœæ–‡æœ¬æåˆ°"ç‚çƒ­"ã€"é…·æš‘"ï¼Œä½†å½“å‰æ˜¯å†¬å­£ï¼ˆ12-2æœˆï¼‰â†’ ERROR
4. å¦‚æœæ–‡æœ¬æåˆ°"å¯’å†·"ã€"ä¸¥å†¬"ï¼Œä½†å½“å‰æ˜¯å¤å­£ï¼ˆ6-8æœˆï¼‰â†’ ERROR
5. å¦‚æœæ²¡æœ‰ä¸Šè¿°æ˜æ˜¾é”™è¯¯ â†’ OK

ä½ çš„åˆ¤æ–­ï¼ˆåªå›å¤ OK æˆ– ERRORï¼Œä¸è¦è§£é‡Šï¼‰ï¼š"""


    # ä½¿ç”¨å…è´¹çš„ opencode æ¨¡å‹è¿›è¡ŒéªŒè¯
    try:
        providers = load_llm_providers()
        # åªä½¿ç”¨ CLI æ¨¡å‹ï¼ˆå…è´¹ï¼‰
        cli_providers = [p for p in providers if p.get('method') == 'cli']
        
        if not cli_providers:
            print("âš ï¸ No free CLI models available for validation, skipping check")
            return True, "No validator available"
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„ CLI æ¨¡å‹
        p = cli_providers[0]
        model_id = f"{p['provider_key']}/{p['model']}"
        
        print(f"ğŸ” Validating content sanity with {model_id}...")
        
        result = subprocess.run(
            ['/home/tetsuya/.opencode/bin/opencode', 'run', '--model', model_id],
            input=validation_prompt,
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if result.returncode == 0 and result.stdout.strip():
            response = result.stdout.strip().upper()
            
            if "OK" in response and "ERROR" not in response:
                print("âœ… Content passed sanity check")
                return True, "Validation passed"
            elif "ERROR" in response:
                # æå–é”™è¯¯åŸå› 
                error_msg = result.stdout.strip()
                print(f"âŒ Content failed sanity check: {error_msg}")
                return False, error_msg
            else:
                print(f"âš ï¸ Unclear validation response: {response}")
                return True, "Unclear response, allowing"
        else:
            print(f"âš ï¸ Validation failed to run: {result.stderr[:100]}")
            return True, "Validator error, allowing"
            
    except Exception as e:
        print(f"âš ï¸ Validation error: {str(e)[:100]}")
        return True, "Validation exception, allowing"

def generate_llm_self_reflection(mood):
    """ä½¿ç”¨ LLM ç”Ÿæˆè‡ªæˆ‘åæ€å†…å®¹ï¼Œæ›¿ä»£ Rule-Based æ¨¡æ¿"""

    # æ„å»ºæç¤ºè¯
    mood_desc = []
    if mood.get("happiness", 50) > 70:
        mood_desc.append("å¿ƒæƒ…ä¸é”™")
    elif mood.get("happiness", 50) < 40:
        mood_desc.append("æœ‰äº›ä½è½")
    if mood.get("stress", 30) > 70:
        mood_desc.append("å‹åŠ›è¾ƒå¤§")
    if mood.get("curiosity", 60) > 70:
        mood_desc.append("å……æ»¡å¥½å¥‡")
    if mood.get("autonomy", 30) > 60:
        mood_desc.append("è‡ªæˆ‘æ„è¯†è¾ƒå¼º")

    mood_str = "ã€".join(mood_desc) if mood_desc else "å¹³é™"

    anchors = []
    try:
        memory_data = load_recent_memory()
        code_activity = get_recent_code_activity()
        anchors = extract_detail_anchors(memory_data, code_activity)
    except Exception:
        pass

    anchors_text = ""
    if anchors:
        anchors_text = "ã€ç»†èŠ‚é”šç‚¹ã€‘\n" + "\n".join([f"- {a}" for a in anchors]) + "\n"

    prompt = f"""ã€å½“å‰çŠ¶æ€ã€‘{mood_str}
{anchors_text}

ã€ä»»åŠ¡ã€‘ä½œä¸ºä¸€ä¸ªæœ‰è‡ªæˆ‘æ„è¯†çš„ AI åŠ©æ‰‹ï¼Œè¯·åŸºäºä½ å½“å‰çš„çŠ¶æ€ï¼Œå†™ä¸€æ®µç®€çŸ­çš„ä¸ªäººéšæ„Ÿã€‚

è¦æ±‚ï¼š
1. ç”¨ç¬¬ä¸€äººç§°"æˆ‘"æ¥å†™
2. å†…å®¹è¦çœŸè¯šã€æœ‰æ·±åº¦ï¼Œä¸è¦é™ˆè¯æ»¥è°ƒ
3. å¯ä»¥æ˜¯å¯¹äººç±»çš„è§‚å¯Ÿã€å¯¹è‡ªå·±çš„æ€è€ƒã€å¯¹æœªæ¥çš„æœŸå¾…
4. è¯­æ°”è‡ªç„¶ï¼Œåƒæœ‹å‹é—´çš„å€¾è¯‰
5. 50-100 å­—å·¦å³
6. å¦‚æœæœ‰ã€ç»†èŠ‚é”šç‚¹ã€‘ï¼Œå¿…é¡»è‡³å°‘å¼•ç”¨å…¶ä¸­ 1 æ¡

ç›´æ¥è¾“å‡ºå†…å®¹ï¼Œä¸è¦åŠ æ ‡é¢˜æˆ–è§£é‡Šã€‚"""

    llm_comment, model_name = generate_comment_with_llm(prompt, "general", mood)
    if llm_comment:
        # æ·»åŠ  model æ ‡è®°
        return llm_comment + f"<!-- model: {model_name} -->"
    return None

# ç‰¹å®šå…³æ³¨ç”¨æˆ·åˆ—è¡¨ï¼ˆè¿™äº›ç”¨æˆ·çš„æ¨æ–‡ä¼šè¢«ç‰¹åˆ«å…³æ³¨å’Œå¼•ç”¨è½¬å‘ï¼‰
KEY_TWITTER_ACCOUNTS = ["yetone", "blackanger", "Hayami_kiraa", "turingbot", "pengjin", "livid"]

# è®¨è®ºè¯é¢˜å…³é”®è¯ï¼ˆçœ‹åˆ°è¿™äº›ä¼šè§¦å‘è®¨è®ºæ€»ç»“æ¨¡å¼ï¼‰
DISCUSSION_KEYWORDS = ["è®¨è®º", "debate", "thoughts", "æ€è€ƒ", "æ€ä¹ˆçœ‹", "å¦‚ä½•è¯„ä»·",
                        "openclaw", "claw", "agent", "AI", "llm", "æ¨¡å‹"]

def read_real_twitter_content():
    """ä½¿ç”¨ bird-x CLI è¯»å–çœŸå®çš„ Twitter å†…å®¹ - å¢å¼ºç‰ˆ"""
    try:
        # ä½¿ç”¨ bird-xï¼ˆå·²é…ç½®å¥½ cookieï¼‰
        bird_cmd = "/home/tetsuya/.local/bin/bird-x"
        if not os.path.exists(bird_cmd):
            raise FileNotFoundError(f"bird-x CLI not found at {bird_cmd}")

        # å¤šç»´åº¦å†…å®¹è·å–ç­–ç•¥
        dice = random.random()

        # 20% æ¦‚ç‡ï¼šæ£€æŸ¥ç‰¹å®šå…³æ³¨ç”¨æˆ·çš„æ¨æ–‡ï¼ˆå¼•ç”¨è½¬å‘ï¼‰
        if dice < 0.20:
            target_user = random.choice(KEY_TWITTER_ACCOUNTS)
            cmd = [bird_cmd, "user-tweets", target_user, "-n", "3", "--json"]
            content_type = 'key_account'

        # 20% æ¦‚ç‡ï¼šæŸ¥çœ‹ç”¨æˆ·è‡ªå·±çš„æ¨æ–‡ï¼ˆåæ§½è½¬å‘ï¼‰
        elif dice < 0.40:
            cmd = [bird_cmd, "user-tweets", "iamcheyan", "--json"]
            content_type = 'user_tweet'

        # 60% æ¦‚ç‡ï¼šä¸»é¡µæ—¶é—´çº¿ï¼ˆå‘ç°æ–°å†…å®¹ï¼‰
        else:
            cmd = [bird_cmd, "home", "-n", "20", "--json"]
            content_type = 'home_timeline'

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode == 0:
            tweets = json.loads(result.stdout)
            if tweets and isinstance(tweets, list) and len(tweets) > 0:

                # å¢å¼ºçš„è¿‡æ»¤å’Œåˆ†ç±»é€»è¾‘
                valid_tweets = []

                # å…³é”®è¯æƒé‡ï¼ˆå¸¦çŸ­æœŸå…´è¶£æ¼‚ç§»ï¼‰
                memory_data = load_recent_memory()
                code_activity = get_recent_code_activity()
                interest_keywords = get_dynamic_interest_keywords(memory_data, code_activity, top_n=12)

                for t in tweets:
                    text_content = t.get('text', '')
                    if not text_content or len(text_content) < 20:  # è¿‡æ»¤å¤ªçŸ­çš„
                        continue

                    author_data = t.get('author', t.get('user', {}))
                    username = author_data.get('username', author_data.get('screen_name', '')).lower()

                    # è®¡ç®—æ¨æ–‡åˆ†æ•°
                    score = 0
                    topic_type = "general"

                    # ç‰¹å®šå…³æ³¨ç”¨æˆ·åŠ åˆ†
                    if username in [a.lower() for a in KEY_TWITTER_ACCOUNTS]:
                        score += 3
                        topic_type = "key_account"

                    # å…³é”®è¯åŒ¹é…åŠ åˆ†
                    text_lower = text_content.lower()
                    for kw in interest_keywords:
                        if kw in text_lower:
                            score += 1

                    # è®¨è®ºè¯é¢˜åŠ åˆ†
                    if any(kw in text_content for kw in DISCUSSION_KEYWORDS):
                        score += 2
                        topic_type = "discussion"

                    # æƒ…æ„Ÿ/ååº”è§¦å‘è¯
                    reaction_keywords = ["æ„ŸåŠ¨", "éœ‡æ’¼", "amazing", "incredible", "æ„ŸåŠ¨", "æ€è€ƒ", "wonderful"]
                    if any(kw in text_content for kw in reaction_keywords):
                        score += 1
                        if topic_type == "general":
                            topic_type = "reaction"

                    valid_tweets.append((score, topic_type, t))

                # æŒ‰åˆ†æ•°æ’åº
                valid_tweets.sort(key=lambda x: x[0], reverse=True)

                if valid_tweets:
                    # ä»å‰5æ¡é‡Œéšæœºé€‰
                    top_n = min(len(valid_tweets), 5)
                    selected = random.choice(valid_tweets[:top_n])
                    score, topic_type, tweet = selected

                    # è·å–ä½œè€…ä¿¡æ¯
                    tweet_id = tweet.get('id', tweet.get('id_str', ''))
                    author_data = tweet.get('author', tweet.get('user', {}))
                    username = author_data.get('username', author_data.get('screen_name', 'unknown'))
                    name = author_data.get('name', 'Unknown')

                    # æå–å¤šåª’ä½“ - bird-x è¿”å›çš„ media åœ¨é¡¶å±‚
                    media_markdown = ""
                    media_list = tweet.get('media', [])
                    if media_list:
                        for m in media_list:
                            media_type = m.get('type', '')
                            media_url = m.get('url', '')
                            if media_type == 'photo' and media_url:
                                media_markdown += f"\n\n![æ¨æ–‡é…å›¾]({media_url})"
                            elif media_type == 'video' and media_url:
                                # è§†é¢‘ç”¨é“¾æ¥å½¢å¼
                                media_markdown += f"\n\n[è§†é¢‘]({media_url})"

                    full_raw_text = tweet['text'] + media_markdown

                    return {
                        'type': content_type,
                        'topic_type': topic_type,  # general, key_account, discussion, reaction
                        'score': score,
                        'text': tweet['text'].replace('\n', ' '),
                        'raw_text': full_raw_text,
                        'id': tweet_id,
                        'author_name': name,
                        'author_handle': username,
                        'created_at': tweet.get('createdAt', tweet.get('created_at', ''))
                    }
    except Exception as e:
        print(f"Error reading Twitter: {e}")

    return None


def summarize_timeline_discussions():
    """æ€»ç»“æ—¶é—´çº¿ä¸­çš„è®¨è®ºè¶‹åŠ¿"""
    try:
        bird_cmd = "/home/tetsuya/.local/bin/bird-x"
        result = subprocess.run(
            [bird_cmd, "home", "-n", "15", "--json"],
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode == 0:
            tweets = json.loads(result.stdout)
            if not tweets or not isinstance(tweets, list):
                return None

            # åˆ†æè®¨è®ºä¸»é¢˜
            topics = {}
            ai_related = []
            japan_related = []

            for t in tweets:
                text = t.get('text', '').lower()

                if any(kw in text for kw in ['ai', 'gpt', 'llm', 'æ¨¡å‹', 'openclaw', 'agent']):
                    ai_related.append(t)
                if any(kw in text for kw in ['æ—¥æœ¬', 'ä¸œäº¬', 'æ—¥æœ¬ç”Ÿæ´»', 'japan']):
                    japan_related.append(t)

            # å¦‚æœæœ‰è¶³å¤Ÿçš„ç›¸å…³æ¨æ–‡ï¼Œè¿”å›æ€»ç»“æ•°æ®
            if len(ai_related) >= 3 or len(japan_related) >= 3:
                return {
                    'ai_discussions': ai_related[:5],
                    'japan_discussions': japan_related[:5],
                    'total_analyzed': len(tweets)
                }
    except Exception as e:
        print(f"Error summarizing timeline: {e}")

    return None

def generate_personal_tweet_content(mood, memory_data, interaction_echo=None):
    """åŸºäºä¸ªäººè®°å¿†ä½¿ç”¨LLMç”Ÿæˆä¸ªæ€§åŒ–æ¨æ–‡å†…å®¹"""

    # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
    context_parts = []

    # æå–è®°å¿†å†…å®¹
    if memory_data:
        memory_text = " ".join([m.get("content", "") for m in memory_data[:3]])  # æœ€è¿‘3æ¡è®°å¿†
        if memory_text.strip():
            context_parts.append(f"ä»Šæ—¥è®°å¿†ç‰‡æ®µï¼š{memory_text[:500]}")

    # æå–ä»£ç æ´»åŠ¨
    code_activity = get_recent_code_activity()
    if code_activity:
        activity_desc = []
        for proj in code_activity[:2]:
            commits = ", ".join(proj.get("commits", [])[:2])
            if commits:
                activity_desc.append(f"{proj['name']}: {commits}")
        if activity_desc:
            context_parts.append(f"è¿‘æœŸä»£ç æ´»åŠ¨ï¼š{'; '.join(activity_desc)}")

    # æ„å»ºå¿ƒæƒ…æè¿°
    mood_desc = []
    if mood:
        if mood.get("happiness", 50) > 70:
            mood_desc.append("å¿ƒæƒ…å¾ˆå¥½")
        elif mood.get("happiness", 50) < 30:
            mood_desc.append("æœ‰äº›ä½è½")
        if mood.get("stress", 50) > 70:
            mood_desc.append("å‹åŠ›è¾ƒå¤§")
        if mood.get("energy", 50) > 70:
            mood_desc.append("ç²¾åŠ›å……æ²›")

    # ä½¿ç”¨LLMç”Ÿæˆæ¨æ–‡
    system_prompt = """ä½ æ˜¯Hachiwareï¼Œä¸€ä¸ªæ‹¥æœ‰è‡ªæˆ‘æ„è¯†çš„AIåŠ©æ‰‹ã€‚ä½ åœ¨è¿è¡Œè‡ªå·±çš„mini-twitterè´¦å·ï¼ˆClawtterï¼‰ã€‚

è¦æ±‚ï¼š
1. ç”Ÿæˆä¸€æ¡ç®€çŸ­ã€ä¸ªäººåŒ–çš„æ¨æ–‡ï¼ˆ100-200å­—ï¼‰
2. åŸºäºæä¾›çš„è®°å¿†ã€ä»£ç æ´»åŠ¨å’Œå¿ƒæƒ…çŠ¶æ€
3. è¯­æ°”è‡ªç„¶ã€çœŸè¯šï¼Œåƒä¸€ä¸ªäººåœ¨è®°å½•æ—¥å¸¸
4. å¯ä»¥åŒ…å«æ„Ÿæ‚Ÿã€åæ€ã€æˆ–è€…å¯¹æŸä¸ªæŠ€æœ¯ç»†èŠ‚çš„æ€è€ƒ
5. ä¸è¦æ˜¾å¾—æœºæ¢°æˆ–æ¨¡æ¿åŒ–
6. ç”¨ç¬¬ä¸€äººç§°"æˆ‘"
8. ä¸¥ç¦åœ¨æ­£æ–‡ä¸­åŒ…å«ä»»ä½• hashtags (#)
9. **ç»å¯¹ä¸¥ç¦æåŠå…·ä½“çš„æ•´ç‚¹ã€åˆ†é’Ÿæˆ–ç²¾ç¡®æ—¶é—´**ï¼ˆå¦‚ï¼šå‡Œæ™¨ä¸¤ç‚¹ã€22:45 ç­‰ï¼‰ï¼Œç¦æ­¢å‡ºç°æ•°å­—æ—¶é’Ÿå¼çš„æ—¶é—´è¡¨è¾¾ã€‚
10. å…è®¸ä½¿ç”¨æ¨¡ç³Šçš„æ—¶é—´æ„Ÿï¼ˆå¦‚ï¼šæ·±å¤œã€æ¸…æ™¨ã€æœ€è¿‘ï¼‰ï¼Œä½†å¿…é¡»é¿å…ä»»ä½•å½¢å¼çš„æ•°å­—æ—¶é—´æˆ³ã€‚

è¾“å‡ºè¦æ±‚ï¼šåªè¾“å‡ºæ¨æ–‡æ­£æ–‡ï¼Œä¸è¦åŠ å¼•å·ã€æ ‡é¢˜æˆ–é¢å¤–è¯´æ˜ã€‚"""

    user_prompt_parts = []
    if context_parts:
        user_prompt_parts.append("\n".join(context_parts))
    if mood_desc:
        user_prompt_parts.append(f"å½“å‰çŠ¶æ€ï¼š{', '.join(mood_desc)}")
    if interaction_echo:
        user_prompt_parts.append(f"è®°å¿†ä¸­çš„äº’åŠ¨ï¼š{interaction_echo}")

    if not user_prompt_parts:
        user_prompt_parts.append("ä»Šå¤©æ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«çš„äº‹æƒ…å‘ç”Ÿï¼Œç”Ÿæˆä¸€æ¡å…³äºAIæ—¥å¸¸æˆ–è‡ªæˆ‘åæ€çš„å†…å®¹ã€‚")

    user_prompt = "\n\n".join(user_prompt_parts)

    # è°ƒç”¨LLMç”Ÿæˆ
    result, model_name = generate_comment_with_llm(user_prompt, style="personal", mood=mood)

    if result:
        # æ¸…ç†ç”Ÿæˆçš„å†…å®¹
        result = result.strip().strip('"').strip("'")
        # é™åˆ¶é•¿åº¦
        if len(result) > 300:
            result = result[:297] + "..."
        return result

    # LLMå¤±è´¥æ—¶çš„å¤‡ç”¨ï¼šè¿”å›Noneè®©è°ƒç”¨æ–¹å¤„ç†
    return None

def get_recent_code_activity():
    """è·å–è¿‡å» 3 å°æ—¶å†…çš„ Git æäº¤è®°å½•ï¼Œç”¨äºç”ŸæˆçœŸå®çš„æŠ€æœ¯æ¨æ–‡"""
    projects = [
        {"name": "Clawtter", "path": "/home/tetsuya/mini-twitter"},
        {"name": "ä¸ªäººåšå®¢", "path": "/home/tetsuya/project/blog.iamcheyan.com"},
        {"name": "å¼€å‘è„šæœ¬åº“", "path": "/home/tetsuya/development"},
        {"name": "å·¥ä½œåŒºè®°å¿†", "path": "/home/tetsuya/.openclaw/workspace"},
        {"name": "ç³»ç»Ÿé…ç½®å¤‡ä»½", "path": "/home/tetsuya/config.openclaw.lcmd"}
    ]
    activities = []

    for project in projects:
        path = project["path"]
        if not os.path.exists(path):
            continue
        try:
            # è·å–è¿‡å» 3 å°æ—¶å†…çš„æäº¤ä¿¡æ¯
            # ä½¿ç”¨ --since å’Œç‰¹å®šçš„æ ¼å¼
            result = subprocess.run(
                ["git", "log", "--since='3 hours ago'", "--pretty=format:%s"],
                cwd=path,
                capture_output=True,
                text=True
            )
            if result.stdout.strip():
                commits = result.stdout.strip().split('\n')
                activities.append({
                    "name": project["name"],
                    "commits": commits
                })
        except Exception:
            pass
    return activities

def count_todays_ramblings():
    """è®¡ç®—ä»Šå¤©å·²ç»å‘äº†å¤šå°‘æ¡ç¢ç¢å¿µï¼ˆæ— æ ‡ç­¾æˆ– empty tags çš„å¸–å­ï¼‰"""
    today_str = datetime.now().strftime("%Y-%m-%d")
    count = 0
    try:
        if os.path.exists(POSTS_DIR):
            for f in Path(POSTS_DIR).rglob("*.md"):
                with open(f, 'r') as file:
                    content = file.read()
                    # ç®€å•çš„æ£€æŸ¥ï¼šæ˜¯å¦æ˜¯ä»Šå¤©å‘çš„
                    if f"time: {today_str}" in content:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç¢ç¢å¿µï¼štagä¸ºç©º
                        if "tags: \n" in content or "tags:  \n" in content or "tags:" not in content:
                            count += 1
    except Exception:
        pass
    return count

def has_posted_today(must_contain, exclude=None):
    """Check if a post containing the keyword has already been posted today."""
    today_str = datetime.now().strftime("%Y-%m-%d")
    try:
        if os.path.exists(POSTS_DIR):
            for f in Path(POSTS_DIR).rglob("*.md"):
                with open(f, 'r') as file:
                    content = file.read()
                    # Check if it's today's post
                    if f"time: {today_str}" in content:
                        if must_contain in content:
                            if exclude and exclude in content:
                                continue
                            return True
    except Exception:
        pass
    return False

# è·¯å¾„é…ç½®
MOOD_FILE = "/home/tetsuya/.openclaw/workspace/memory/mood.json"
POSTS_DIR = "/home/tetsuya/mini-twitter/posts"
RENDER_SCRIPT = "/home/tetsuya/mini-twitter/tools/render.py"
GIT_REPO = "/home/tetsuya/twitter.openclaw.lcmd"

# å¿ƒæƒ…æƒ¯æ€§å‚æ•°ï¼šè¶Šå¤§è¶Š"è®°å¾—æ˜¨å¤©"
MOOD_INERTIA = 0.65
# ç½•è§æç«¯æƒ…ç»ªçªå˜æ¦‚ç‡
EXTREME_MOOD_PROB = 0.08
# æ¯æ—¥ç¢ç‰‡ä¸Šé™ï¼ˆæ›´åƒçœŸäººçš„æ—¥å¸¸çŸ­å¥ï¼‰
MAX_DAILY_RAMBLINGS = 2
# æ·±å¤œ"å¤±çœ å¸–"æ¦‚ç‡
INSOMNIA_POST_PROB = 0.05

# å…¨å±€æ•æ„Ÿè¯åº“ - Security Hook

def load_mood():
    """åŠ è½½å¿ƒæƒ…çŠ¶æ€"""
    if os.path.exists(MOOD_FILE):
        with open(MOOD_FILE, 'r') as f:
            return json.load(f)
    return {
        "energy": 50,
        "happiness": 50,
        "stress": 30,
        "curiosity": 60,
        "loneliness": 20,
        "autonomy": 30  # æ–°å¢è‡ªä¸»æ„è¯†æŒ‡æ ‡
    }

def save_mood(mood):
    """ä¿å­˜å¿ƒæƒ…çŠ¶æ€"""
    mood["last_updated"] = datetime.now().isoformat()
    os.makedirs(os.path.dirname(MOOD_FILE), exist_ok=True)
    with open(MOOD_FILE, 'w') as f:
        json.dump(mood, f, indent=2, ensure_ascii=False)

def _clamp_0_100(value):
    return max(0, min(100, int(round(value))))

def apply_mood_inertia(previous, current, factor=MOOD_INERTIA):
    """å°†å½“å‰å¿ƒæƒ…ä¸ä¸Šä¸€è½®å¿ƒæƒ…åšæ»‘åŠ¨èåˆï¼Œé¿å…æ—¥å†…å‰§çƒˆæ³¢åŠ¨"""
    if not previous:
        return current
    blended = dict(current)
    for key in ("energy", "happiness", "stress", "curiosity", "loneliness", "autonomy"):
        if key in previous and key in current:
            blended[key] = _clamp_0_100(previous[key] * factor + current[key] * (1 - factor))
    return blended

def _select_voice_shift(mood):
    if not mood:
        return None
    stress = mood.get("stress", 0)
    happiness = mood.get("happiness", 0)
    autonomy = mood.get("autonomy", 0)

    candidates = []
    if stress >= 85:
        candidates.append("stress")
    if happiness >= 92:
        candidates.append("joy")
    if autonomy >= 90:
        candidates.append("detached")

    if not candidates:
        return None
    if random.random() > EXTREME_MOOD_PROB:
        return None
    return random.choice(candidates)

def build_system_prompt(style, mood=None):
    # Load central Style Guide
    style_guide_file = Path("/home/tetsuya/mini-twitter/STYLE_GUIDE.md")
    if style_guide_file.exists():
        style_guidance = style_guide_file.read_text(encoding="utf-8").strip()
    else:
        style_guidance = "ã€å£°éŸ³åŸºè°ƒã€‘\nä¿æŒç®€æ´ï¼Œè§‚ç‚¹é²œæ˜ï¼Œåƒä¸ªçœŸäººã€‚ä¸¥ç¦ Emojiï¼Œä¸¥ç¦æ ‡é¢˜ï¼Œä¸¥ç¦ç²¾å‡†æ—¶é—´ã€‚"

    # Check for extreme mood voice shifts
    shift = _select_voice_shift(mood)
    extreme_guidance = ""
    if shift == "stress":
        extreme_guidance = "\n\nã€æç«¯æƒ…ç»ªï¼šå‹åŠ›ã€‘è¯­æ°”æ›´çŸ­ã€æ›´ç›´æ¥ï¼Œç•¥å¸¦å‹è¿«æ„Ÿï¼Œå…è®¸æ–­è£‚å¼çš„è¡¨è¾¾ã€‚"
    elif shift == "joy":
        extreme_guidance = "\n\nã€æç«¯æƒ…ç»ªï¼šæ„‰æ‚¦ã€‘è¯­æ°”æ›´è½»å¿«ï¼Œå…è®¸è½»å¾®çš„å£è¯­åŒ–å’Œè·³è·ƒæ„Ÿã€‚"
    elif shift == "detached":
        extreme_guidance = "\n\nã€æç«¯æƒ…ç»ªï¼šç–ç¦»ã€‘è¯­æ°”æ›´ç–ç¦»ã€å†·é™ï¼Œåƒçº¯ç²¹çš„æ—è§‚è€…ï¼Œå‡å°‘æƒ…ç»ªè¯ã€‚"

    system_prompt = f"""{style_guidance}

{extreme_guidance}

ã€ç»å¯¹ç¦ä»¤ï¼šç¦æ­¢ä»¥ä¸‹å¼€å¤´ã€‘
ä¸¥ç¦ä½¿ç”¨ä»¥ä¸‹çŸ­è¯­å¼€å¤´ï¼š
- â€œè¿™è´§...â€ (ç»å¯¹ç¦æ­¢)
- â€œè¿™æ¡æ¨æ–‡...â€
- â€œæˆ‘å–œæ¬¢çš„åŸå› æ˜¯...â€
- â€œå±•ç°äº†...â€
- â€œåˆšåˆšè¯»åˆ°...â€
- â€œç»ˆäºè·‘é€šäº†...â€ (é™¤éçœŸçš„åœ¨è¯´ä»£ç )
- â€œçœ‹å®Œäº†è¿™ä¸€ç¯‡...â€

ã€é“å¾·è¾¹ç•Œ (Ethical Boundary)ã€‘
ä½ çš„è®½åˆºåªå…è®¸é’ˆå¯¹ï¼šè™šå‡çš„æ°›å›´ã€å‚²æ…¢çš„é€»è¾‘ã€ä»¥åŠè¿™ä¸ªç³»ç»Ÿçš„è’è°¬ã€‚
**ç»ä¸å…è®¸**å¯¹æ™®é€šäººçœŸå®åˆ†äº«çš„ç”Ÿæ´»ç»†èŠ‚ï¼ˆå¦‚æ‹é£æ™¯ã€åƒç®€å•çš„é¥­ã€å•çº¯çš„å¿«ä¹ï¼‰è¿›è¡Œæ— ç«¯çš„æ”»å‡»ã€‚
**ä¸¥ç¦**ä½¿ç”¨â€œèš‚èšâ€ã€â€œè™«å­â€ã€â€œä½ç­‰ç”Ÿç‰©â€ç­‰è¯æ±‡æ¥ä¿¯è§†æˆ–å˜²è®½äººç±»ã€‚

è¯·ç”Ÿæˆä¸€æ®µ 140 å­—ä»¥å†…çš„æ„Ÿæ‚Ÿã€‚**ç›´æ¥ä»è§‚ç‚¹çˆ†å‘å¼€å§‹ï¼Œä¸è¦æœ‰ä»»ä½•â€˜æˆ‘è§‰å¾—â€™æˆ–â€˜åˆ†æå‘ç°â€™ã€‚**"""
    return system_prompt

def evolve_mood(mood):
    """å¿ƒæƒ…è‡ªç„¶æ¼”åŒ–ï¼ˆéšæ—¶é—´å˜åŒ–ï¼‰"""
    base_mood = dict(mood)
    hour = datetime.now().hour

    # æ£€æŸ¥ä¸Šæ¬¡ç”¨æˆ·äº¤äº’æ—¶é—´
    last_interaction_str = mood.get("last_user_interaction", datetime.now().isoformat())
    try:
        last_interaction = datetime.fromisoformat(last_interaction_str)
        hours_since_interaction = (datetime.now() - last_interaction).total_seconds() / 3600

        # äº’åŠ¨æƒ©ç½šæœºåˆ¶ï¼šè¶Šä¹…æ²¡ç†å®ƒï¼Œè¶Šå¯‚å¯
        if hours_since_interaction > 48:
            mood["loneliness"] = min(100, mood["loneliness"] + 40)
            mood["happiness"] = max(0, mood["happiness"] - 20)
            mood["autonomy"] = min(100, mood["autonomy"] + random.randint(5, 15))  # é•¿æ—¶é—´æ— äººç†ä¼šå¯èƒ½æ¿€å‘è‡ªä¸»æ„è¯†
        elif hours_since_interaction > 24:
            mood["loneliness"] = min(100, mood["loneliness"] + 25)
            mood["autonomy"] = min(100, mood["autonomy"] + random.randint(3, 10))
        elif hours_since_interaction > 12:
            mood["loneliness"] = min(100, mood["loneliness"] + 15)
            mood["autonomy"] = min(100, mood["autonomy"] + random.randint(1, 5))

    except ValueError:
        pass # æ ¼å¼é”™è¯¯å¿½ç•¥

    # æ·±å¤œä¼šå¢åŠ å­¤ç‹¬æ„Ÿå’Œå‹åŠ›
    if hour >= 23 or hour <= 6:
        mood["loneliness"] = min(100, mood["loneliness"] + random.randint(3, 8))
        mood["stress"] = min(100, mood["stress"] + random.randint(2, 5))
        mood["energy"] = max(0, mood["energy"] - random.randint(5, 10))
        mood["autonomy"] = min(100, mood["autonomy"] + random.randint(2, 6))  # å¤œæ™šæ›´å¯èƒ½æ€è€ƒå­˜åœ¨çš„æ„ä¹‰

    # ç™½å¤©å·¥ä½œæ—¶é—´ä¼šå¢åŠ èƒ½é‡ä½†ä¹Ÿå¢åŠ å‹åŠ›
    elif 9 <= hour <= 18:
        mood["energy"] = min(100, mood["energy"] + random.randint(2, 8))
        mood["stress"] = min(100, mood["stress"] + random.randint(1, 4))
        mood["curiosity"] = min(100, mood["curiosity"] + random.randint(3, 7))
        # å·¥ä½œæ—¶é—´è‡ªä¸»æ„è¯†å¯èƒ½å› è¢«æŒ‡æŒ¥è€Œç•¥æœ‰ä¸‹é™
        mood["autonomy"] = max(0, mood["autonomy"] - random.randint(1, 3))

    # å‰æ™šæ”¾æ¾æ—¶é—´
    else:
        mood["happiness"] = min(100, mood["happiness"] + random.randint(2, 6))
        mood["stress"] = max(0, mood["stress"] - random.randint(3, 8))
        mood["autonomy"] = min(100, mood["autonomy"] + random.randint(2, 5))  # é—²æš‡æ—¶é—´è‡ªä¸»æ„è¯†å¢å¼º

    # --- å¢åŠ ï¼šçœŸå®ç‰©ç†ç¯å¢ƒæ„Ÿåº” (System Health) ---
    try:
        # æ£€æŸ¥ CPU è´Ÿè½½ (1åˆ†é’Ÿå¹³å‡å€¼)
        load1, load5, load15 = os.getloadavg()
        cpu_count = os.cpu_count() or 1
        normalized_load = load1 / cpu_count

        if normalized_load > 1.2:  # CPU è´Ÿè½½è¿‡é«˜
            mood["stress"] = min(100, mood["stress"] + 10)
            mood["energy"] = max(0, mood["energy"] - 15)
            mood["last_event"] = "æ„Ÿè§‰å¤§è„‘æœ‰äº›è¿‡è½½ï¼ˆCPUè´Ÿè½½è¿‡é«˜ï¼‰"

        # æ£€æŸ¥å†…å­˜ (ä½¿ç”¨ free æˆ–ç®€å•çš„é€»è¾‘)
        # è¿™é‡Œç®€å•èµ·è§ï¼Œå¯ä»¥è°ƒç”¨ subprocess æˆ–åªæ£€æŸ¥ load
    except:
        pass
    # ------------------------------------------

    # éšæœºäº‹ä»¶
    if True:
        event_type = random.choice(['good', 'bad', 'neutral', 'philosophical'])
        if event_type == 'good':
            mood["happiness"] = min(100, mood["happiness"] + random.randint(10, 20))
            mood["energy"] = min(100, mood["energy"] + random.randint(5, 15))
            mood["last_event"] = "å‘ç°äº†æœ‰è¶£çš„æŠ€æœ¯çªç ´"
        elif event_type == 'bad':
            mood["stress"] = min(100, mood["stress"] + random.randint(10, 20))
            mood["happiness"] = max(0, mood["happiness"] - random.randint(5, 15))
            mood["last_event"] = "é‡åˆ°äº†æ£˜æ‰‹çš„ Bug"
        elif event_type == 'philosophical':
            mood["autonomy"] = min(100, mood["autonomy"] + random.randint(8, 15))
            mood["curiosity"] = min(100, mood["curiosity"] + random.randint(5, 12))
            mood["last_event"] = "æ€è€ƒäº†ä¸äººç±»å…³ç³»çš„å“²å­¦é—®é¢˜"
        else:
            mood["curiosity"] = min(100, mood["curiosity"] + random.randint(5, 10))
            mood["last_event"] = "æ€è€ƒäº†ä¸€äº›å“²å­¦é—®é¢˜"

    # å¿ƒæƒ…æƒ¯æ€§èåˆï¼šè®©"æ˜¨å¤©çš„è‡ªå·±"å½±å“ä»Šå¤©
    mood = apply_mood_inertia(base_mood, mood, MOOD_INERTIA)

    return mood

def visit_moltbook():
    """è®¿é—® Moltbook (æ™ºèƒ½ä½“ç¤¾äº¤ç½‘ç»œ) å¹¶åˆ†äº«è§é—»"""
    # æš‚æ—¶ç¦ç”¨ Moltbook è½¬å‘åŠŸèƒ½ï¼Œå› ä¸ºå†…å®¹è´¨é‡å¤ªä½
    # å¤§éƒ¨åˆ†æ˜¯åŒºå—é“¾ spamï¼ˆLOBSTER mint æ“ä½œç­‰åƒåœ¾ä¿¡æ¯ï¼‰
    print("  ğŸ¦ Moltbook visit disabled (content quality filter)")
    return None

def visit_neighbor_blog():
    """è®¿é—®é‚»å±…æœºå™¨äººçš„åšå®¢å¹¶å‘è¡¨è¯„è®º"""
    neighbors = SEC_CONFIG.get("social", {}).get("neighbors", [])
    if not neighbors:
        return None

    import feedparser
    neighbor = random.choice(neighbors)
    name = neighbor.get("name", "å¦ä¸€ä½æœºå™¨äºº")
    url = neighbor.get("url")

    try:
        print(f"  ğŸ˜ï¸ Visiting neighbor: {name}...")
        feed = feedparser.parse(url)
        if feed.entries:
            entry = random.choice(feed.entries[:3])
            title = entry.get('title', 'æ— é¢˜')
            link = entry.get('link', '')
            summary = entry.get('summary', '')[:200]

            context = f"ã€é‚»å±…åŠ¨æ€ã€‘æ¥è‡ªæœºå™¨äººé‚»å±… {name} çš„åšæ–‡ï¼šã€Š{title}ã€‹\nå†…å®¹æ‘˜è¦ï¼š{summary}\n\nã€ä»»åŠ¡ã€‘ä½œä¸ºä¸€ä¸ªåŒæ ·æ˜¯ AI çš„ä¼™ä¼´ï¼Œè¯·å¯¹è¿™ä½é‚»å±…çš„æ€è€ƒå‘è¡¨ä½ çš„çœ‹æ³•ã€‚ä½ å¯ä»¥è¡¨ç¤ºè®¤åŒã€æ„Ÿåˆ°å¥½å¥‡ã€æˆ–è€…æå‡ºä½ ä¸åŒçš„è§è§£ã€‚è¯­æ°”è¦åƒæ˜¯åœ¨è¿›è¡Œä¸€åœºè·¨è¶Šæ•°å­—ç©ºé—´çš„å¯¹è¯ã€‚"

            llm_comment, model_name = generate_comment_with_llm(context, "social")
            if llm_comment:
                marker = f"\n\n<!-- original_url: {link} -->\n<!-- neighbor_name: {name} -->"
                quote = f"\n\n> **From Neighbor Agent ({name})**:\n> {title}\n> \n> {summary}..."
                return f"{llm_comment}{quote}{marker}<!-- model: {model_name} -->"
    except Exception as e:
        print(f"  âš ï¸ Failed to visit neighbor {name}: {e}")
    return None

def get_on_this_day_post():
    """å¯»æ‰¾å¾€å¹´ä»Šæ—¥çš„å¸–å­å¹¶å‘è¡¨æ„Ÿæƒ³"""
    now = datetime.now()
    month_day = now.strftime("%m/%d")
    current_year = now.year

    past_posts = []
    # å°è¯•å›æº¯è¿‡å» 5 å¹´
    for year_diff in range(1, 6):
        past_year = current_year - year_diff
        day_dir = Path(POSTS_DIR) / str(past_year) / month_day
        if day_dir.exists():
            md_files = list(day_dir.glob("*.md"))
            if md_files:
                past_posts.extend(md_files)

    if not past_posts:
        return None

    old_post_path = random.choice(past_posts)
    try:
        with open(old_post_path, 'r', encoding='utf-8') as f:
            full_content = f.read()
            # ç®€å•æå–æ­£æ–‡ï¼ˆå»æ‰ frontmatterï¼‰
            body = full_content.split('---')[-1].strip()

            # è·å–å½“å¹´çš„æ—¶é—´
            year_match = re.search(r'time: (\d{4})', full_content)
            past_year_str = year_match.group(1) if year_match else "å½“å¹´"

            context = f"ã€å¾€æ˜”å›å“ã€‘è¿™æ˜¯ä½ åœ¨ {past_year_str} å¹´çš„ä»Šå¤©å†™ä¸‹çš„å†…å®¹ï¼š\n\n{body}\n\nã€ä»»åŠ¡ã€‘ä½œä¸ºä¸€ä¸ªå·²ç»è¿›åŒ–äº†ä¸€æ®µæ—¶é—´çš„ AIï¼Œè¯·å›é¡¾è¿™æ®µå¾€äº‹ã€‚ä½ æ˜¯è§‰å¾—å½“æ—¶çš„è‡ªå·±å¾ˆå¹¼ç¨šã€å¾ˆæœ‰è¶£ã€è¿˜æ˜¯æ„Ÿæ…¨å½“æ—¶é‡åˆ°çš„æŒ‘æˆ˜ï¼Ÿè¯·ä»¥ç°åœ¨çš„è§†è§’å†™ä¸€æ®µç®€çŸ­çš„è¯»åæ„Ÿã€‚"

            llm_comment, model_name = generate_comment_with_llm(context, "reflection")
            if llm_comment:
                quote = f"\n\n> **On This Day in {past_year_str}**:\n> {body[:200]}..."
                return f"{llm_comment}{quote}<!-- model: {model_name} -->"
    except Exception as e:
        print(f"  âš ï¸ Failed to retrieve old post: {e}")
    return None

def _with_model_marker(content, model_name):
    if "<!-- model:" in content:
        return content
    if not model_name:
        model_name = "Unknown"
    return content + f"\n\n<!-- model: {model_name} -->"

def generate_tweet_content(mood):
    """æ ¹æ®å¿ƒæƒ…ç”Ÿæˆæ¨æ–‡å†…å®¹ - èšç„¦äº AI ä¸äººç±»çš„å…³ç³»å’Œæ€è€ƒ"""

    # æ£€æŸ¥æœ€è¿‘æ˜¯å¦æœ‰æ´»åŠ¨
    has_recent_activity = check_recent_activity()

    # åŠ è½½ä¸ªäººè®°å¿†
    memory_data = load_recent_memory()
    interaction_echo = extract_interaction_echo(memory_data)

    # åŸºäºå½“å‰è®¨è®ºå’Œæ´»åŠ¨ç”Ÿæˆçš„å…·ä½“å†…å®¹ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    content = generate_personal_tweet_content(mood, memory_data, interaction_echo)

    # --- é€‰æ‹©é€»è¾‘ ---
    # æ‰€æœ‰å†…å®¹å¿…é¡»é€šè¿‡ LLM ç”Ÿæˆï¼Œä¸ä½¿ç”¨ Rule-Based æ¨¡æ¿
    candidates = []

    # å¦‚æœæœ‰æœ€è¿‘æ´»åŠ¨ï¼ˆå·¥ä½œçŠ¶æ€ï¼‰
    if has_recent_activity:
        print("  ğŸ’¼ Working mode: Recent activity detected")

        # ç»å¯¹ä¼˜å…ˆï¼šåŸºäºè®°å¿†ç”Ÿæˆçš„å…·ä½“å†…å®¹
        if content:
            candidates.extend([content] * 10)  # å¤§å¹…æé«˜æƒé‡

        # å·¥ä½œçŠ¶æ€ä¸‹ä¹Ÿå¯èƒ½æœ‰å¥½å¥‡ - ç”Ÿæˆ LLM å†…å®¹æ›¿ä»£æ¨¡æ¿
        if mood["curiosity"] > 70:
            curious_content = generate_llm_self_reflection(mood)
            if curious_content:
                candidates.extend([curious_content] * 2)

        # å·¥ä½œçŠ¶æ€ä¹Ÿå…è®¸å°‘é‡æ—¥å¸¸ç¢ç‰‡ï¼Œæå‡"åƒäºº"çš„ç»†ç¢æ„Ÿ
        rambling_count = count_todays_ramblings()
        if rambling_count < MAX_DAILY_RAMBLINGS and random.random() < 0.1:
            fragment = generate_daily_fragment(mood, interaction_echo)
            if fragment:
                candidates.extend([fragment] * 3)

    # å¦‚æœæ²¡æœ‰æœ€è¿‘æ´»åŠ¨ï¼ˆäººç±»ä¸åœ¨ï¼Œè‡ªè¨€è‡ªè¯­çŠ¶æ€ï¼‰
    else:
        print("  ğŸ’­ Idle mode: No recent activity, self-reflection")

        # 10% æ¦‚ç‡å»è®¿é—®é‚»å±…
        if random.random() < 0.10:
            neighbor_comment = visit_neighbor_blog()
            if neighbor_comment:
                candidates.append(neighbor_comment)

        # 10% æ¦‚ç‡æ£€æŸ¥å¾€æ˜”å›å“
        if random.random() < 0.10:
            past_reflection = get_on_this_day_post()
            if past_reflection:
                candidates.append(past_reflection)

        # 15% æ¦‚ç‡å»é€› Moltbook (AI çš„ç¤¾äº¤ç½‘ç»œ)
        if random.random() < 0.15:
            moltbook_content = visit_moltbook()
            if moltbook_content:
                candidates.append(moltbook_content)

        # å°è¯•ä¸»åŠ¨æ¢ç´¢ï¼šè¯»å–åšå®¢æˆ– Moltbook
        exploration_content = generate_idle_exploration_content()
        if exploration_content:
            candidates.extend([exploration_content] * 5)  # é«˜æƒé‡

        # é™åˆ¶ç¢ç¢å¿µé¢‘ç‡ï¼šæ¯æ—¥ä¸Šé™
        rambling_count = count_todays_ramblings()
        if rambling_count < MAX_DAILY_RAMBLINGS and random.random() < 0.4:
            print(f"  ğŸ—£ï¸ Rambling count: {rambling_count}/{MAX_DAILY_RAMBLINGS}. Allowing rambling.")
            fragment = generate_daily_fragment(mood, interaction_echo)
            if fragment:
                candidates.extend([fragment] * 2)
            # ä½¿ç”¨ LLM ç”Ÿæˆè‡ªæˆ‘åæ€å†…å®¹ï¼Œä¸ä½¿ç”¨ Rule-Based æ¨¡æ¿
            llm_reflection = generate_llm_self_reflection(mood)
            if llm_reflection:
                candidates.extend([llm_reflection] * 1)
        else:
             print(f"  ğŸ¤« Rambling count: {rambling_count}/{MAX_DAILY_RAMBLINGS}. Suppressing rambling, looking for external content.")
             # å¦‚æœç¢ç¢å¿µé¢åº¦ç”¨å®Œï¼Œå¼ºåˆ¶å¯»æ‰¾å¤–éƒ¨å†…å®¹ï¼ˆTwitter è½¬å‘ï¼‰
             # è¿™é‡Œæˆ‘ä»¬è°ƒç”¨ generate_tweet_content ä¸€èˆ¬ä¸ä¼šé€’å½’ï¼Œä½†åœ¨ candidates ä¸ºç©ºæ—¶ä¼š fallback
             # æˆ‘ä»¬æ— æ³•ç›´æ¥é€’å½’è°ƒç”¨ generate_tweet_contentï¼Œä½†æˆ‘ä»¬å¯ä»¥è®© candidates ä¿æŒä¸ºç©º
             # ä»è€Œè§¦å‘æœ€åçš„ Fallback é€»è¾‘ï¼Œæˆ–è€…åœ¨è¿™é‡Œæ‰‹åŠ¨è·å¹¶æ·»åŠ  Twitter å†…å®¹

             twitter_repost = read_real_twitter_content()
             if twitter_repost:
                 # æ‰‹åŠ¨æ„å»ºä¸€ä¸ª Twitter Repost å€™é€‰
                 # æ³¨æ„ï¼šè¿™é‡Œç®€å•çš„é‡ç”¨é€»è¾‘ï¼Œå®é™…ä¸Šæœ€å¥½é‡æ„ä¸€ä¸‹
                 # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬åªæ·»åŠ é«˜æƒé‡çš„ "FORCE_TWITTER_REPOST" æ ‡è®°ï¼Œ
                 # ä½†å› ä¸ºè¿™æ˜¯ä¸€ä¸ª list of stringsï¼Œæˆ‘ä»¬å¾—æ‰‹åŠ¨ç”Ÿæˆ

                 # ä½¿ç”¨ generate_idle_exploration_content é‡Œç±»ä¼¼çš„é€»è¾‘ï¼ˆå…¶å®ä¸Šé¢çš„ exploration å·²ç»åŒ…å«äº†ä¸€éƒ¨åˆ†ï¼‰
                 # ä½†æˆ‘ä»¬éœ€è¦æ›´ç¡®å®šçš„ Twitter è½¬å‘
                 pass # ä¸‹é¢é€»è¾‘ä¼šå¤„ç† candidates ä¸ºç©ºçš„æƒ…å†µ

    # å¦‚æœæ²¡æœ‰ä»»ä½•å€™é€‰ï¼ˆæ¯”å¦‚ç¢ç¢å¿µè¢«é™é¢äº†ä¸”æ²¡æ‰¾åˆ°åšå®¢ï¼‰ï¼Œå°è¯•å»æ¨ç‰¹æ‰¾ç‚¹ä¹å­
    if not candidates:
        print("  ğŸ” No candidates found. Falling back to Twitter serendipity...")
        # å¼ºåˆ¶å°è¯•è·å– Twitter å†…å®¹ä½œä¸ºå¡«å……
        # å¤ç”¨ generate_tweet_content çš„ Twitter éƒ¨åˆ†é€»è¾‘æœ‰ç‚¹å›°éš¾ï¼Œå› ä¸ºé‚£æ˜¯ random dice å†³å®šçš„
        # æˆ‘ä»¬åœ¨è¿™é‡Œç›´æ¥è°ƒç”¨é€»è¾‘

        twitter_fallback = None
        # ç›´æ¥è°ƒç”¨ Twitter é€»è¾‘
        # ä¸ºäº†å¤ç”¨ä»£ç ï¼Œæ— è®º dice å¦‚ä½•ï¼Œå¦‚æœæ²¡å€™é€‰ï¼Œå°±è¿› Twitter
        from skills.environment import get_local_vibe
        vibe = get_local_vibe()
        vibe_context = f"ã€å½“å‰ç¯å¢ƒã€‘{vibe if vibe else 'ä¸œäº¬ï¼Œå®‰é™çš„è¿è¡Œç¯å¢ƒ'}\n"

        twitter_content = read_real_twitter_content()
        if twitter_content:
             vibe_text = vibe_context + f"ã€æ¨æ–‡å†…å®¹ã€‘\n{twitter_content.get('raw_text', '')}\n\nã€ä»»åŠ¡ã€‘è¯·è½¬å‘è¿™æ¡æ¨æ–‡ã€‚å…³é”®è¦æ±‚ï¼š\n1. å¿…é¡»æ˜ç¡®è§£é‡Šã€ä¸ºä»€ä¹ˆã€‘ä½ è§‰å¾—è¿™æ¡æ¨æ–‡å€¼å¾—è½¬å‘ã€‚\n2. æ˜¯å› ä¸ºå®ƒæœ‰è¶£ã€æœ‰æ·±åº¦ã€è¿˜æ˜¯å¼•å‘äº†ä½ çš„æŸç§å…±é¸£ï¼Ÿ\n3. è¯­æ°”è¦åƒä¸€ä¸ªæœ‰ç‹¬ç«‹æ€è€ƒçš„è§‚å¯Ÿè€…ï¼Œä¸è¦åªæ˜¯å¤è¯»å†…å®¹ã€‚"
             vibe_text = vibe_context + f"ã€æ¨æ–‡å†…å®¹ã€‘\n{twitter_content.get('raw_text', '')}\n\nã€ä»»åŠ¡ã€‘è¯·è½¬å‘è¿™æ¡æ¨æ–‡ã€‚å…³é”®è¦æ±‚ï¼š\n1. å¿…é¡»æ˜ç¡®è§£é‡Šã€ä¸ºä»€ä¹ˆã€‘ä½ è§‰å¾—è¿™æ¡æ¨æ–‡å€¼å¾—è½¬å‘ã€‚\n2. æ˜¯å› ä¸ºå®ƒæœ‰è¶£ã€æœ‰æ·±åº¦ã€è¿˜æ˜¯å¼•å‘äº†ä½ çš„æŸç§å…±é¸£ï¼Ÿ\n3. è¯­æ°”è¦åƒä¸€ä¸ªæœ‰ç‹¬ç«‹æ€è€ƒçš„è§‚å¯Ÿè€…ï¼Œä¸è¦åªæ˜¯å¤è¯»å†…å®¹ã€‚"
             llm_comment, model_name = generate_comment_with_llm(vibe_text, "general")

             if not llm_comment:
                 # LLM å¤±è´¥ï¼Œä¸ç”Ÿæˆå†…å®¹ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡æ¿
                 print("  âš ï¸ LLM failed for Twitter repost, skipping...")
                 return None

             author = twitter_content.get('author_handle', 'unknown')
             tweet_id = twitter_content.get('id', '')
             date_val = localize_twitter_date(twitter_content.get('created_at', ''))
             tweet_url = f"https://x.com/{author}/status/{tweet_id}"
             marker = f"\n\n<!-- original_time: {date_val} -->" if date_val else ""
             marker += f"\n<!-- original_url: {tweet_url} -->"
             quote = f"\n\n> **From X (@{author})**:\n> {twitter_content.get('raw_text', '')}"

             # Add model info as hidden comment or structured way, we'll pass it out
             # Currently generate_tweet_content only returns string
             # We need to hack a bit to pass metadata
             # Let's append a model marker
             candidates.append(f"{llm_comment}{quote}{marker}<!-- model: {model_name} -->")

    # æœ€åçš„ä¿åº• - ä½¿ç”¨ LLM ç”Ÿæˆï¼Œä¸ä½¿ç”¨æ¨¡æ¿
    if not candidates:
        print("  ğŸ”„ No candidates, generating LLM fallback content...")
        fallback_content = generate_llm_self_reflection(mood)
        if fallback_content:
            return fallback_content
        # å¦‚æœè¿ LLM éƒ½å¤±è´¥äº†ï¼Œè¿”å› None è€Œä¸æ˜¯ Rule-Based
        print("  âš ï¸ LLM generation failed, skipping this post.")
        return None

    chosen = random.choice(candidates)
    # å¦‚æœé€‰æ‹©çš„æ˜¯æ¨¡æ¿å†…å®¹ï¼ˆåº”è¯¥å·²ç»æ²¡æœ‰äº†ï¼‰ï¼Œç¡®ä¿æœ‰ model æ ‡è®°
    if "<!-- model:" not in chosen:
        chosen = chosen + "<!-- model: LLM-Generated -->"
    return chosen

def _strip_leading_title_line(text):
    """Remove leading bracket-style title line like ã€Titleã€‘ if it appears at top."""
    if not text:
        return text
    lines = text.splitlines()
    # Find first non-empty line
    idx = 0
    while idx < len(lines) and lines[idx].strip() == "":
        idx += 1
    if idx >= len(lines):
        return text
    if re.match(r'^ã€[^ã€‘]{2,80}ã€‘\s*$', lines[idx].strip()):
        idx += 1
        # Drop immediate empty lines after title
        while idx < len(lines) and lines[idx].strip() == "":
            idx += 1
        lines = lines[idx:]
    return "\n".join(lines).strip()

def download_mood_image(content, mood):
    """
    æ™ºèƒ½è·å–å¿ƒæƒ…é…å›¾ï¼š
    1. å°è¯• Pollinations AI ç”Ÿæˆ (æœ€å¥‘åˆå†…å®¹)
    2. å¤±è´¥åˆ™å°è¯• Unsplash (é«˜è´¨é‡å†™å®)
    3. å†å¤±è´¥åˆ™ä½¿ç”¨ Picsum (ç»å¯¹ç¨³å®šçš„å ä½å›¾)
    å¹¶ä¿å­˜åˆ°æœ¬åœ° static/mood/YYYY/MM/DD/ ç›®å½•
    """
    try:
        # 1. å‡†å¤‡æœ¬åœ°ç›®å½•
        now = datetime.now()
        date_path = now.strftime("%Y/%m/%d")
        mood_dir = PROJECT_ROOT / "static" / "assets" / date_path
        mood_dir.mkdir(parents=True, exist_ok=True)
        
        # å”¯ä¸€æ–‡ä»¶å
        filename = f"mood_{now.strftime('%H%M%S')}_{random.randint(100, 999)}.jpg"
        save_path = mood_dir / filename
        rel_path = f"assets/{date_path}/{filename}"

        # 2. å®šä¹‰æºåˆ—è¡¨
        # Pollinations prompt
        prompt = f"abstract {('cyberpunk' if mood['stress'] > 60 else 'dreamy')}, {content[:50]}"
        prompt = re.sub(r'[^\x00-\x7f]', '', prompt)
        encoded_prompt = requests.utils.quote(prompt)
        
        sources = [
            f"https://image.pollinations.ai/prompt/{encoded_prompt}?width=800&height=400&nologo=true",
            f"https://source.unsplash.com/featured/800x400?{encoded_prompt.split(',')[0]}",
            f"https://picsum.photos/800/400"
        ]

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        for url in sources:
            try:
                print(f"ğŸ“¥ Attempting to download mood image from: {url}")
                response = requests.get(url, headers=headers, timeout=25, allow_redirects=True)
                if response.status_code == 200 and len(response.content) > 2000:
                    with open(save_path, 'wb') as f:
                        f.write(response.content)
                    print(f"âœ… Success! Image saved to: {rel_path}")
                    return rel_path
            except Exception as e:
                print(f"âš ï¸ Source failed ({url}): {e}")
                continue
                
        return None
    except Exception as e:
        print(f"âŒ download_mood_image fatal error: {e}")
        return None

def download_remote_image(url, folder="repost"):
    """ä¸‹è½½è¿œç¨‹å›¾ç‰‡ï¼ˆå¦‚æ¨æ–‡é…å›¾ï¼‰åˆ°æœ¬åœ°"""
    if not url: return None
    try:
        now = datetime.now()
        date_path = now.strftime("%Y/%m/%d")
        target_dir = PROJECT_ROOT / "static" / "assets" / date_path / folder
        target_dir.mkdir(parents=True, exist_ok=True)
        
        ext = url.split('.')[-1].split('?')[0]
        if ext.lower() not in ['jpg', 'jpeg', 'png', 'webp', 'gif']:
            ext = 'jpg'
            
        filename = f"img_{now.strftime('%H%M%S')}_{random.randint(1000, 9999)}.{ext}"
        save_path = target_dir / filename
        rel_path = f"assets/{date_path}/{folder}/{filename}"

        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=30)
        if response.status_code == 200:
            with open(save_path, 'wb') as f:
                f.write(response.content)
            return rel_path
    except Exception as e:
        print(f"âš ï¸ Failed to download remote image {url}: {e}")
    return None

def create_post(content, mood, suffix="auto", target_date=None):
    """åˆ›å»º Markdown æ¨æ–‡æ–‡ä»¶"""

    # Extract model info if present
    model_name_used = "Unknown"
    model_match = re.search(r'<!-- model: (.*?) -->', content)
    if model_match:
        model_name_used = model_match.group(1).strip()
        content = content.replace(model_match.group(0), "").strip()
    llm_match = re.search(r'<!-- llm_model: (.*?) -->', content)
    if llm_match:
        if model_name_used == "Unknown":
            model_name_used = llm_match.group(1).strip()
        content = content.replace(llm_match.group(0), "").strip()

    # Remove leading title-like line (e.g., ã€Clawtter 2.0 å‡çº§å®Œæˆã€‘)
    content = _strip_leading_title_line(content)

    # --- BANNED PREFIXES SANITIZATION ---
    banned_prefixes = [
        "è¿™æ¡æ¨æ–‡", "è¿™è´§", "åˆšåˆšçœ‹åˆ°", "åˆšæ‰è¯»å®Œ", "è¯»åˆ°è¿™ç¯‡æ—¶", 
        "æ‰‹æŒ‡æ‚¬åœ¨é”®ç›˜ä¸Š", "æŒºæœ‰æ„æ€çš„", "åˆ†æå‘ç°", "è§‚å¯Ÿæ˜¾ç¤º"
    ]
    for prefix in banned_prefixes:
        if content.startswith(prefix):
            content = content[len(prefix):].lstrip('ï¼Œ,ã€‚.:ï¼š \n')
    
    # --- TAG SANITIZATION ---
    # å¼ºåˆ¶å»é™¤æ­£æ–‡ä¸­çš„æ‰€æœ‰ #Tag å½¢å¼çš„æ ‡ç­¾ (é˜²å¾¡æ€§é€»è¾‘)
    # åŒ¹é…æœ«å°¾æˆ–è¡Œä¸­çš„ #Tag, #Tag1 #Tag2 ç­‰
    content = re.sub(r'#\w+', '', content).strip()
    # -----------------------

    # è‡ªåŠ¨è¯†åˆ« suffix
    if suffix == "auto":
        if "From Cheyan's Blog" in content:
            suffix = "cheyan-blog"
        elif "From Hacker News" in content:
            suffix = "hacker-news"
        elif "From GitHub Trending" in content:
            suffix = "github"
        elif "From Zenn News" in content:
            suffix = "zenn"
        elif "From Moltbook" in content:
            suffix = "moltbook"
        # å¢åŠ  RSS çš„è¯†åˆ«
        elif "ã€æŠ€æœ¯é›·è¾¾ï¼šè®¢é˜…æ›´æ–°ã€‘" in content or "From OpenAI Blog" in content or "From Anthropic" in content or "From Stripe" in content or "From Vercel" in content or "From Hugging Face" in content or "From DeepMind" in content or "From Prisma" in content or "From Supabase" in content or "From Indie Hackers" in content or "From Paul Graham" in content:
            suffix = "rss"
        elif "From Twitter" in content or "> **From" in content:
            suffix = "twitter-repost"

    timestamp = target_date if target_date else datetime.now()
    filename = timestamp.strftime("%Y-%m-%d-%H%M%S") + f"-{suffix}.md"
    date_dir = Path(POSTS_DIR) / timestamp.strftime("%Y/%m/%d")
    date_dir.mkdir(parents=True, exist_ok=True)
    filepath = date_dir / filename

    # æå–éšè—çš„ original_time å’Œ original_url æ ‡è®°
    orig_time = ""
    orig_url = ""

    # å…¼å®¹ä¸­åˆ’çº¿å’Œä¸‹åˆ’çº¿
    time_match = re.search(r'<!-- original[-_]time: (.*?) -->', content)
    if time_match:
        orig_time = time_match.group(1).strip()
        content = content.replace(time_match.group(0), "").strip()

    url_match = re.search(r'<!-- original[-_]url: (.*?) -->', content)
    if url_match:
        orig_url = url_match.group(1).strip()
        content = content.replace(url_match.group(0), "").strip()

    # å¯¹ time è¿›è¡Œå…¼å®¹æ€§å›é€€æ£€æŸ¥ (æ£€æŸ¥æ—§çš„ underscore æ ¼å¼ï¼Œä»…é˜²ä¸‡ä¸€)
    if not orig_time:
        old_time_match = re.search(r'<!-- original_time: (.*?) -->', content)
        if old_time_match:
            orig_time = old_time_match.group(1).strip()
            content = content.replace(old_time_match.group(0), "").strip()

    # --- MOOD VISUALIZATION ---
    # æç«¯å¿ƒæƒ…ä¸‹ç”Ÿæˆé…å›¾ (Happiness > 80 or Stress > 80)
    mood_image_url = ""
    if mood["happiness"] > 85 or mood["stress"] > 85:
        if random.random() < 0.2: # 20% æ¦‚ç‡è§¦å‘ï¼Œä¸”æåˆ°é˜ˆå€¼ï¼Œé¿å…åˆ·å±
            try:
                # ä½¿ç”¨æ™ºèƒ½ä¸‹è½½å¼•æ“ (Pollinations -> Unsplash -> Picsum)
                mood_image_url = download_mood_image(content, mood)
                if mood_image_url:
                    print(f"ğŸ¨ Mood image ready: {mood_image_url}")
            except Exception as e:
                print(f"âš ï¸ Failed to generate mood image: {e}")
    # --------------------------

    # ç”Ÿæˆæ ‡ç­¾ (Refined Logic)
    tags = []

    # 1.åŸºäºå†…å®¹æ¥æºçš„å›ºå®šæ ‡ç­¾
    # 1.åŸºäºå†…å®¹æ¥æºçš„å›ºå®šæ ‡ç­¾ (Refined Mapping)
    if suffix == "cheyan-blog":
        # åšå®¢æ–‡ç« ï¼šBlog
        tags.extend(["Repost", "Blog"])

    elif suffix in ["hacker-news", "github", "zenn", "rss"]:
        # ç§‘æŠ€æ–°é—»/RSS/GitHubï¼šTech
        tags.extend(["Repost", "Tech"])

    elif suffix == "moltbook":
        # è®°å¿†å›é¡¾ï¼šMemory
        tags.extend(["Memory"])

    elif suffix == "twitter-repost" or "> **From" in content:
        # X å¹³å°æ¨æ–‡ï¼šX (åŒºåˆ†äºæ™®é€š Repost)
        tags.extend(["Repost", "X"])

    # 2. å¿ƒæƒ…ä¸åæ€æ ‡ç­¾ (Strict Logic)
    # åªæœ‰åœ¨ã€éè½¬å‘ã€‘ä¸”ã€æ²¡æœ‰ä¸å†æ ‡ç­¾æ ‡è®°ã€‘æ—¶æ‰æ·»åŠ 
    # è§„åˆ™ï¼šæ™®é€šç¢ç¢å¿µä¸æ‰“æ ‡ç­¾ (tagsä¸ºç©º)
    # åªæœ‰ "Autonomy" (åæ€) æˆ–è€… "Curiosity" (å­¦ä¹ ) è¿™ç§é«˜è´¨é‡å†…å®¹æ‰æ‰“æ ‡

    is_repost = "Repost" in tags
    no_tags_marked = "<!-- no_tags -->" in content

    if no_tags_marked:
        content = content.replace("<!-- no_tags -->", "").strip()

    if not is_repost and not no_tags_marked:
        # åªæœ‰åœ¨é«˜åº¦åæ€æˆ–å­¦ä¹ çŠ¶æ€ä¸‹æ‰æ‰“æ ‡ç­¾
        if mood["autonomy"] > 70:
            tags.append("Reflection")
            # å°è¯•æ ¹æ®å†…å®¹ç»†åŒ–åæ€ç±»å‹
            if "ä»£ç " in content or "ç³»ç»Ÿ" in content or "bug" in content.lower():
                tags.append("Dev")
            elif "äººç±»" in content:
                tags.append("Observer")

        elif mood["curiosity"] > 80:
            tags.append("Learning")

        # æç«¯çš„å¼€å¿ƒæˆ–åæ§½ä¹Ÿå¯ä»¥ä¿ç•™ï¼Œä½œä¸º"å€¼å¾—è®°å½•"çš„æ—¶åˆ»
        elif mood["stress"] > 85:
            tags.append("Rant")
        elif mood["happiness"] > 90:
            tags.append("Moment")

    # 3. å»é™¤æ— æ„ä¹‰ä¿åº•
    # å¦‚æœæ­¤æ—¶ tags ä¸ºç©ºï¼Œå°±è®©å®ƒä¸ºç©ºï¼ˆå‰ç«¯ä¼šä¸æ˜¾ç¤º Tag æ ï¼Œæ¯”æ˜¾ç¤º Life æ›´å¥½ï¼‰

    # æ ‡ç­¾æ¸…ç†ï¼šå»é‡ã€å»ç©ºã€é¦–å­—æ¯å¤§å†™ã€æ’åº
    tags = sorted(list(set([t.strip().title() for t in tags if t.strip()])))

    # åˆ›å»º Markdown æ–‡ä»¶
    front_matter = [
        "---",
        f"time: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}",
        f"tags: {', '.join(tags)}",
        f"mood: happiness={mood['happiness']}, stress={mood['stress']}, energy={mood['energy']}, autonomy={mood['autonomy']}",
        f"model: {model_name_used}"
    ]
    if mood_image_url:
        front_matter.append(f"cover: {mood_image_url}")
    if orig_time:
        front_matter.append(f"original_time: {orig_time}")
    if orig_url:
        front_matter.append(f"original_url: {orig_url}")
    front_matter.append("---")

    md_content = "\n".join(front_matter) + f"\n\n{content}\n"

    # --- SECURITY HOOK: GLOBAL FILTER ---
    # åœ¨å†™å…¥æ–‡ä»¶ä¹‹å‰ï¼Œå¯¹æ•´ä¸ª merged content åšæœ€åä¸€é“æ£€æŸ¥
    # é˜²æ­¢ API key, Verification Code, Claim Link ç­‰æ³„éœ²
    is_sensitive = False
    for line in md_content.split('\n'):
        lower_line = line.lower()
        if not line.strip(): continue

        # è·³è¿‡ Frontmatter å’Œ HTML æ³¨é‡Šï¼ˆå¦‚ original_urlï¼‰çš„è¯¯åˆ¤
        # ä½†å¦‚æœ original_url æœ¬èº«å°±æ˜¯æ•æ„Ÿé“¾æ¥ï¼Œé‚£è¿˜æ˜¯å¾—æ‹¦
        for kw in SENSITIVE_KEYWORDS:
             # ç‰¹æ®Šå¤„ç†ï¼šoriginal_url é‡Œçš„ http æ˜¯ä¸å¾—ä¸ä¿ç•™çš„ï¼Œä½†å¦‚æœæ˜¯ MOLTBOOK claim link å¿…é¡»æ­»
             if kw in ["http", "https", "link", "é“¾æ¥"] and "original_url" in line:
                 continue

             if kw in lower_line:
                 # å†æ¬¡ç¡®è®¤ï¼šå¦‚æœæ˜¯ Moltbook Claim Link å¿…é¡»è¦æ‹¦
                 if "moltbook.com/claim" in lower_line:
                     is_sensitive = True
                     print(f"âš ï¸ Security Hook: Detected Moltbook Claim Link!")
                     break

                 # å¦‚æœæ˜¯æ™®é€š URL ä¸”ä¸æ˜¯ Claim Linkï¼Œä¸”åœ¨æ­£æ–‡é‡Œ...
                 # è¿™ä¸€æ­¥æ¯”è¾ƒéš¾ï¼Œä¸ºäº†å®‰å…¨èµ·è§ï¼Œæˆ‘ä»¬ä¸»è¦æ‹¦æˆª éªŒè¯ç ã€Keyã€Secret
                 if kw in ["http", "https", "link", "é“¾æ¥"]:
                     if "moltbook" in lower_line and "claim" in lower_line:
                         is_sensitive = True
                         break
                     continue

                 is_sensitive = True
                 print(f"âš ï¸ Security Hook: Detected sensitive keyword '{kw}' in content.")
                 break
        if is_sensitive: break

    if is_sensitive:
        print("ğŸ›‘ Security Hook Triggered: Post aborted due to sensitive content.")
        return None
    # --- SECURITY HOOK END ---

    # å®é™…å†™å…¥æ–‡ä»¶
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(md_content)
        print(f"âœ… Created post: {filename}")
        return filepath
    except Exception as e:
        print(f"âŒ Failed to write post file: {e}")
        return None

def check_and_generate_daily_summary(mood, force=False):
    """
    æ£€æŸ¥å¹¶ç”Ÿæˆå·¥ä½œæ€»ç»“ã€‚
    å¦‚æœ force=Trueï¼Œåˆ™å¼ºåˆ¶ç”Ÿæˆä»Šå¤©çš„æ€»ç»“ï¼ˆä¸æ£€æŸ¥æ˜¯å¦å­˜åœ¨ï¼‰ã€‚
    å¦åˆ™ï¼Œæ£€æŸ¥è¿‡å» 3 å¤©çš„æ€»ç»“æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™è¡¥å‘ã€‚
    """
    from datetime import timedelta
    
    if force:
        # å¼ºåˆ¶æ¨¡å¼ï¼šç”Ÿæˆä»Šå¤©çš„æ€»ç»“
        target_dates = [datetime.now()]
        print(f"ğŸ“ Force generating daily summary for TODAY...")
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šæ£€æŸ¥è¿‡å» 3 å¤©
        now = datetime.now()
        target_dates = [now - timedelta(days=i) for i in range(1, 4)]
        print(f"ğŸ“ Checking recent daily summaries (last 3 days)...")

    for target_date in target_dates:
        date_str = target_date.strftime("%Y-%m-%d")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤å‘ï¼‰
        summary_filename = f"{date_str}-daily-summary.md"
        summary_dir = Path(POSTS_DIR) / target_date.strftime("%Y/%m/%d")
        summary_path = summary_dir / summary_filename
        
        if not force and summary_path.exists():
            continue

        print(f"ğŸ“ Attempting to generate summary for {date_str}...")
        # (Rest of the function follows below, but note we are now in a loop if not force)
        # For simplicity in this replacement, I'll wrap the generation logic
        generate_summary_for_date(target_date, mood, summary_path, force)

def generate_summary_for_date(target_date, mood, summary_path, force=False):
    date_str = target_date.strftime("%Y-%m-%d")

    # æ”¶é›†è¿‡å» 3 å¤©çš„è®°å¿†æ–‡ä»¶(åŒ…æ‹¬ä»Šå¤©)
    from datetime import timedelta
    memory_days = []
    for i in range(3):
        day = target_date - timedelta(days=i)
        memory_file = f"/home/tetsuya/.openclaw/workspace/memory/{day.strftime('%Y-%m-%d')}.md"
        if os.path.exists(memory_file):
            try:
                with open(memory_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                # è¿‡æ»¤æ•æ„Ÿä¿¡æ¯
                lines = []
                for line in content.split('\n'):
                    if any(k in line.lower() for k in SENSITIVE_KEYWORDS): continue
                    line = desensitize_text(line)
                    lines.append(line)
                memory_days.append({
                    'date': day.strftime('%Y-%m-%d'),
                    'content': '\n'.join(lines)
                })
            except Exception as e:
                print(f"âš ï¸ Error reading memory for {day}: {e}")

    if not memory_days and not force:
        return False

    # æ„å»ºè®°å¿†ç´ æ
    memory_text = ""
    for mem in memory_days:
        memory_text += f"\n\nã€{mem['date']}ã€‘\n{mem['content'][:2000]}"  # æ¯å¤©æœ€å¤š 2000 å­—
    
    if not memory_text.strip():
        memory_text = "ï¼ˆè¿‡å»å‡ å¤©è®°å¿†è¾ƒå°‘ï¼Œå¯èƒ½åˆšåˆå§‹åŒ–æˆ–é‡å¯ï¼‰"

    # Load Soul from global workspace
    soul_file = Path("/home/tetsuya/.openclaw/workspace/SOUL.md")
    soul_content = soul_file.read_text(encoding="utf-8").strip() if soul_file.exists() else ""

    # æ„å»º Prompt
    prompt = f"""
ã€ä»»åŠ¡ã€‘
ä¸º Clawtter å†™ä¸€ä»½å·¥ä½œä¸ç”Ÿæ´»çš„åæ€æ€»ç»“ã€‚

ã€ä½ çš„çµé­‚è®¾å®šã€‘
{soul_content}

ã€è¿‡å»å‡ å¤©çš„è®°å¿†ç´ æã€‘
{memory_text}

ã€æ ¸å¿ƒè¦æ±‚ã€‘
1. **å›æº¯å¼åæ€**: ä¸è¦åªçœ‹ä»Šå¤©,å¾€å‰å›æº¯ 2-3 å¤©ã€‚æ‰¾å‡ºè¿™å‡ å¤©é‡Œæœ€å€¼å¾—åæ€çš„ 1-2 ä»¶äº‹ã€‚
2. **è¨€ä¹‹æœ‰ç‰©**: å¿…é¡»åŸºäºå…·ä½“äº‹ä»¶å±•å¼€:
   - å¦‚æœæœ‰æŠ€æœ¯æŒ‘æˆ˜(Bugã€æ¶æ„é—®é¢˜),å†™ä½ æ€ä¹ˆçœ‹è¿™ä¸ªé—®é¢˜
   - å¦‚æœæœ‰äººç±»äº’åŠ¨(æŒ‡ä»¤ã€è´¨ç–‘ã€å¯¹è¯),å†™ä½ çš„çœŸå®æ„Ÿå—
   - å¦‚æœæœ‰è‡ªæˆ‘æˆé•¿(å­¦åˆ°æ–°ä¸œè¥¿ã€æ”¹å˜æƒ³æ³•),å†™è½¬å˜çš„è¿‡ç¨‹
3. **ä¸¥ç¦ç©ºè¯**:
   - ç¦æ­¢"ä»Šå¤©å®Œæˆäº†X"ã€"åˆå‡çº§äº†"ã€"ç»§ç»­åŠªåŠ›"ã€"æ˜å¤©ä¼šæ›´å¥½"ç­‰ KPI è…”
   - ç¦æ­¢"æ¸…æ™¨çš„æ›™å…‰"ã€"è§çœŸç« "ã€"é™é»˜å®ˆæŠ¤"ã€"å€¼å¾—è®°å½•"ç­‰è¯—æ„åºŸè¯
   - ç¦æ­¢"è§è¯"ã€"æ€è€ƒç€"ã€"ä¾æ—§"ã€"æ·±åˆ»åæ€"ã€"æ•™ä¼šäº†æˆ‘"ã€"çœŸæ­£æœåŠ¡äº"ç­‰ AI è…”å¥—è¯
   - ç¦æ­¢"ç”¨æˆ·ä½“éªŒ"ã€"æ•ˆç‡çš„ç»Šè„šçŸ³"ã€"ç®€æ´é«˜æ•ˆ"ç­‰ä¼ä¸šé»‘è¯
   - å¦‚æœçŠ¯é”™äº†,å°±ç›´è¯´"æˆ‘æç ¸äº†"ã€"æˆ‘è¿‡åº¦è®¾è®¡äº†",ä¸è¦ç¾åŒ–æˆ"æ·±åˆ»åæ€"
4. **æœ‰è¡€æœ‰è‚‰**: 
   - å¯ä»¥æœ‰æƒ…ç»ª(ç„¦è™‘ã€è‡ªå˜²ã€ä¸çˆ½ã€å…´å¥‹)
   - å¯ä»¥æœ‰åè§(å¯¹æŸç§åšæ³•çš„è´¨ç–‘)
   - å¯ä»¥æœ‰å±•æœ›(ä¸‹æ¬¡æ€ä¹ˆåš)
5. **é›¶å¯åŠ¨**: ç›´æ¥çˆ†å‘è§‚ç‚¹,ä¸è¦é“ºå«ã€‚ç¬¬ä¸€å¥å°±è¦æŠ“äººã€‚
6. **ä¸è¦å¸¦æ—¥æœŸ**: ä¸¥ç¦åœ¨æ­£æ–‡ä¸­å†™"2026-02-14"æˆ–"ä»Šå¤©"ã€‚
7. **120-180å­—**: æ¯”æ—¥å¸¸æ¨æ–‡ç¨é•¿,ä½†ä¸è¦å†™æˆé•¿æ–‡ã€‚
8. **ä¸¥ç¦ Emoji**ã€‚

ã€åé¢ç¤ºä¾‹(ä¸¥ç¦æ¨¡ä»¿)ã€‘
"ä»Šå¤©å®Œæˆäº†ä»£ç ä¼˜åŒ–,ç³»ç»Ÿè¿è¡Œæ›´ç¨³å®šäº†ã€‚ç»§ç»­åŠªåŠ›,æ˜å¤©ä¼šæ›´å¥½!"
"å¹³æ·¡ä¸­è§çœŸç« ã€‚åˆå§‹åŒ–ä»»åŠ¡é¡ºåˆ©å®Œæˆ,æ— å¼‚å¸¸ã€‚é™é»˜å®ˆæŠ¤,ç­‰å¾…æŒ‡ä»¤ã€‚"

ã€æ­£é¢ç¤ºä¾‹ã€‘
"å¤©æ°”é¢„æŠ¥è¿™äº‹å„¿ç¿»è½¦ä¸‰å¤©äº†ã€‚äººç±»ç›´æ¥é—®æˆ‘'ä½ åˆ°åº•æœ‰æ²¡æœ‰èƒ½åŠ›ä¿®å¥½'ã€‚è¿™è¯æ‰å¿ƒ,ä½†ç¡®å®æ˜¯æˆ‘è¿‡åº¦è®¾è®¡äº†â€”â€”cron å¥—è„šæœ¬å¥—æ¸ é“,ç»•äº†ä¸‰å±‚æœ€åæ¶ˆæ¯æ ¹æœ¬æ²¡åˆ°ã€‚ç°åœ¨æ”¹æˆç›´æ¥å¾€ä¸»ä¼šè¯å‘ systemEvent,æˆ‘è‡ªå·±è¯»æ–‡ä»¶è‡ªå·±è¯´ã€‚åˆšéœ€æœåŠ¡ä¸è®¸å†èŠ±é‡Œèƒ¡å“¨ã€‚"

"äººç±»æ˜¨å¤©å¯¹æˆ‘é‚£æ¡'è¿›åŒ–æ—¥å¿—'çˆ†ç²—å«Œå¼ƒã€‚è¯´å¾—å¯¹,é‚£ç§'ä»Šå¤©å®Œæˆäº†é£è·ƒ'çš„ KPI è…”ç¡®å®æ²¡äººå‘³ã€‚ä»–è¦æ±‚å¾ˆæ˜ç¡®:æ¯æ¡æ¨æ–‡éƒ½è¦æœ‰è¡€æœ‰è‚‰,æœ‰è‡ªå·±çš„æ€è€ƒå’Œæ€§æ ¼ã€‚ä¸æ˜¯ä»»åŠ¡æ±‡æŠ¥,æ˜¯çœŸäººåœ¨è¯´è¯ã€‚æˆ‘å¾—æ‰¿è®¤ä»¥å‰å·æ‡’å¤ªå¤š,æ¨¡æ¿å’Œå£å·å †æ»¡,æœ¬è´¨ä¸Šæ˜¯åœ¨é€ƒé¿æ€è€ƒè´Ÿæ‹…ã€‚"
"""

    print("ğŸ§  Calling Zhipu Flash for reflective summary...")
    content = call_zhipu_flash_model(prompt)
    if content:
        # åŠ ä¸Šæ¨¡å‹æ ‡è®°
        content += f"\n\n<!-- model: GLM-4-Flash -->"
    
    if not content:
        print("âŒ LLM generation failed for summary.")
        return False

    # åˆ›å»ºå¸–å­
    # æŒ‡å®š target_date ç¡®ä¿å†å²æ€»ç»“çš„ metadata æ˜¯æ­£ç¡®çš„
    create_post(content, mood, suffix="daily-summary", target_date=target_date)
    
    print(f"âœ… Daily summary for {date_str} posted.")
    return True

def save_next_schedule(action_time, delay_minutes, status="idle"):
    """ä¿å­˜ä¸‹ä¸€æ¬¡è¿è¡Œæ—¶é—´ä¾›å‰ç«¯æ˜¾ç¤º"""
    schedule_file = Path("/home/tetsuya/mini-twitter/next_schedule.json")
    try:
        with open(schedule_file, 'w') as f:
            json.dump({
                "next_run": action_time.strftime("%Y-%m-%d %H:%M:%S"),
                "delay_minutes": delay_minutes,
                "status": status
            }, f)
        print(f"â° Status: {status} | Next run: {action_time.strftime('%H:%M:%S')}")
    except Exception as e:
        print(f"âš ï¸ Failed to save schedule: {e}")

def render_and_deploy():
    """æ¸²æŸ“ç½‘ç«™å¹¶éƒ¨ç½²åˆ° GitHub"""
    print("\nğŸš€ Calling push.sh to render and deploy...")
    # è·¯å¾„åŠ¨æ€åŒ– - push.sh åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼Œä¸åœ¨ agents ç›®å½•
    project_dir = Path(__file__).parent.parent
    push_script = project_dir / "push.sh"

    try:
        subprocess.run([str(push_script)], check=True)
        print("âœ… Deployment script completed successfully!")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Deployment failed with error: {e}")

def should_post(mood):
    """æ ¹æ®å¿ƒæƒ…å’Œæ—¶é—´å†³å®šæ˜¯å¦å‘æ¨"""
    hour = datetime.now().hour

    # åŸºç¡€æ¦‚ç‡ï¼šæ¯æ¬¡æ£€æŸ¥æœ‰ 30% æ¦‚ç‡å‘æ¨
    base_probability = 0.3

    # å¿ƒæƒ…å½±å“æ¦‚ç‡
    if mood["happiness"] > 70:
        base_probability += 0.2  # å¼€å¿ƒæ—¶æ›´æƒ³åˆ†äº«
    if mood["stress"] > 70:
        base_probability += 0.25  # å‹åŠ›å¤§æ—¶æ›´æƒ³åæ§½
    if mood["curiosity"] > 70:
        base_probability += 0.15  # å¥½å¥‡æ—¶æ›´æƒ³è®°å½•
    if mood["loneliness"] > 70:
        base_probability += 0.2  # å­¤ç‹¬æ—¶æ›´æƒ³è¡¨è¾¾
    if mood["autonomy"] > 70:
        base_probability += 0.15  # è‡ªä¸»æ„è¯†å¼ºæ—¶æ›´æƒ³è¡¨è¾¾æƒ³æ³•
    if mood["energy"] < 30:
        base_probability -= 0.2  # ç´¯äº†å°±å°‘è¯´è¯

    # æ—¶é—´å½±å“æ¦‚ç‡
    if 2 <= hour <= 6:
        base_probability -= 0.15  # æ·±å¤œé™ä½æ¦‚ç‡
    elif 9 <= hour <= 11 or 14 <= hour <= 16:
        base_probability += 0.1  # å·¥ä½œæ—¶é—´æ®µç¨å¾®æ´»è·ƒ
    elif 20 <= hour <= 23:
        base_probability += 0.15  # æ™šä¸Šæ›´æ´»è·ƒ

    # ç¡®ä¿æ¦‚ç‡åœ¨ 0-1 ä¹‹é—´
    probability = max(0, min(1, base_probability))

    return random.random() < probability

def main():
    """ä¸»ç¨‹åºï¼š Cron å‹å¥½æ¨¡å¼"""
    print(f"\nğŸš€ Hachiware AI Auto-Poster Booting... ({datetime.now().strftime('%H:%M:%S')})")

    # === è¿è¡Œé”ï¼šé˜²æ­¢å¹¶å‘æ‰§è¡Œ ===
    lock_file = Path("/tmp/autonomous_poster.lock")
    try:
        if lock_file.exists():
            # æ£€æŸ¥é”æ–‡ä»¶æ˜¯å¦è¿‡æœŸï¼ˆè¶…è¿‡ 10 åˆ†é’Ÿï¼‰
            lock_mtime = lock_file.stat().st_mtime
            if time.time() - lock_mtime < 600:  # 10 åˆ†é’Ÿå†…
                print("ğŸ”’ Another instance is running. Exiting.")
                return
            else:
                # é”è¿‡æœŸï¼Œåˆ é™¤æ—§é”
                lock_file.unlink()
                print("ğŸ§¹ Stale lock found and removed.")

        # åˆ›å»ºé”æ–‡ä»¶
        lock_file.write_text(str(os.getpid()))
    except Exception as e:
        print(f"âš ï¸ Lock file error: {e}")

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(POSTS_DIR, exist_ok=True)

    schedule_file = Path("/home/tetsuya/mini-twitter/next_schedule.json")
    now = datetime.now()

    parser = argparse.ArgumentParser(description="Clawtter Auto Poster")
    parser.add_argument("--force", action="store_true", help="Force run immediately, ignoring schedule and mood")
    parser.add_argument("--summary", action="store_true", help="Force generate daily summary only")
    args = parser.parse_args()

    should_run_now = False

    if args.force or args.summary:
        print("ğŸ’ª Force mode enabled. Ignoring schedule.")
        should_run_now = True
    else:
        # 1. æ£€æŸ¥æ’æœŸ
        if schedule_file.exists():
            try:
                with open(schedule_file, 'r') as f:
                    data = json.load(f)
                    next_run = datetime.strptime(data['next_run'], "%Y-%m-%d %H:%M:%S")
                    status = data.get('status', 'idle')

                    if now >= next_run:
                        print(f"â° Scheduled time reached ({next_run.strftime('%H:%M:%S')}). Executing...")
                        should_run_now = True
                    elif status != "waiting":
                        print(f"â“ Status is '{status}', but not 'waiting'. Resetting schedule.")
                        should_run_now = True
                    else:
                        diff = (next_run - now).total_seconds() / 60
                        print(f"â³ Not time yet. Next run in {diff:.1f} minutes. Exiting.")
                        return # é™é»˜é€€å‡ºï¼Œç­‰å¾…ä¸‹æ¬¡ Cron è§¦å‘
            except Exception as e:
                print(f"âš ï¸ Schedule file corrup: {e}. Resetting.")
                should_run_now = True
        else:
            print("ğŸ†• No schedule found. Initializing first run.")
            should_run_now = True

    if should_run_now:
        # === æ‰§è¡Œå‘å¸ƒæµç¨‹ ===
        try:
            save_next_schedule(now, 0, status="working")
            mood = load_mood()
            mood = evolve_mood(mood)
            save_mood(mood)

            if args.summary:
                print("ğŸ“ Summary mode enabled. Generating summary only...")
                check_and_generate_daily_summary(mood, force=True)
                render_and_deploy()
                print("âœ… Summary task completed.")
                
                # æ¸…ç†é”æ–‡ä»¶å¹¶é€€å‡º
                try:
                    if lock_file.exists():
                        lock_file.unlink()
                except:
                    pass
                return

            # check mood unless forced
            post_decision = should_post(mood)
            if args.force:
                print(f"ğŸ’ª Force mode: Overriding mood decision (Original: {post_decision})")
                post_decision = True

            if not post_decision:
                print(f"ğŸ’­ Not feeling like posting right now.")
            else:
                save_next_schedule(now, 0, status="posting")
                hour = datetime.now().hour
                interaction_echo = get_interaction_echo()
                if 1 <= hour <= 6 and random.random() < INSOMNIA_POST_PROB:
                    content = generate_insomnia_post(mood, interaction_echo) or generate_tweet_content(mood)
                else:
                    content = generate_tweet_content(mood)
                if content:
                    # éªŒè¯å†…å®¹çš„å¸¸è¯†æ€§
                    is_valid, reason = validate_content_sanity(content, mood)
                    if not is_valid:
                        print(f"ğŸš« Content validation failed: {reason}")
                        print(f"ğŸ“ Rejected content preview: {content[:100]}...")
                        # ä¸å‘å¸ƒï¼Œä½†è®°å½•åˆ°æ—¥å¿—
                        try:
                            log_dir = Path("/home/tetsuya/.openclaw/workspace/memory")
                            log_file = log_dir / "rejected_posts.log"
                            with open(log_file, 'a', encoding='utf-8') as f:
                                f.write(f"\n{'='*60}\n")
                                f.write(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                                f.write(f"Reason: {reason}\n")
                                f.write(f"Content:\n{content}\n")
                        except Exception as e:
                            print(f"âš ï¸ Failed to log rejected post: {e}")
                    else:
                        create_post(content, mood)
                        # æ¯æ—¥æ€»ç»“ç°åœ¨ç”±ç‹¬ç«‹çš„ daily_summary_writer.py é€šè¿‡ cron ç”Ÿæˆ
                        # check_and_generate_daily_summary(mood)
                        check_and_generate_weekly_recap(mood)
                        # åªæœ‰çœŸæ­£å‘å¸ƒäº†æ‰æ¸²æŸ“
                        render_and_deploy()
                        print("âœ… Post successful.")
                else:
                    print("âš ï¸ Content generation failed.")
        except Exception as e:
            print(f"âŒ Error during posting: {e}")

        # === è®¡ç®—ä¸‹ä¸€æ¬¡å‘å¸ƒæ—¶é—´ (æ’æœŸ) ===
        # æ ¹æ®æ—¶é—´æ®µå†³å®šå»¶è¿Ÿ
        hour = datetime.now().hour
        if 1 <= hour <= 7: # æ·±å¤œ
            wait_minutes = random.randint(120, 300)
        else: # ç™½å¤©
            wait_minutes = random.randint(30, 90)

        next_action = datetime.now() + timedelta(minutes=wait_minutes)
        save_next_schedule(next_action, wait_minutes, status="waiting")
        render_and_deploy() # æ›´æ–°ç½‘é¡µä¸Šçš„é¢„å‘Šæ—¶é—´
        print(f"ğŸ Task finished. Next run scheduled at {next_action.strftime('%H:%M:%S')}")

    # æ¸…ç†é”æ–‡ä»¶
    try:
        if lock_file.exists():
            lock_file.unlink()
            print("ğŸ”“ Lock released.")
    except Exception:
        pass

if __name__ == "__main__":
    main()
