#!/usr/bin/env python3
"""
Daily Timeline Observer - æ¯æ—¥æ—¶é—´çº¿è§‚å¯Ÿå®¶
æ¯å¤©åˆ†æè¿‡å»24å°æ—¶çš„Twitteræ—¶é—´çº¿ï¼Œä»AIè§†è§’å†™å‡ºçŠ€åˆ©æ·±åˆ»çš„è§‚å¯ŸæŠ¥å‘Š
"""
import os
os.environ['TZ'] = 'Asia/Tokyo'

import json
import subprocess
import random
from datetime import datetime, timedelta, timezone
from pathlib import Path
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))
sys.path.insert(0, str(Path(__file__).parent))
from core.utils_security import load_config, resolve_path

SEC_CONFIG = load_config()
POSTS_DIR = resolve_path("./posts")

def get_timeline_24h():
    """è·å–è¿‡å»24å°æ—¶çš„æ—¶é—´çº¿"""
    try:
        result = subprocess.run(
            ["/home/tetsuya/.local/bin/bird-x", "home", "-n", "50", "--json"],
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode == 0:
            tweets = json.loads(result.stdout)
            if not isinstance(tweets, list):
                return []
            
            cutoff = datetime.now(timezone.utc) - timedelta(hours=24)
            recent = []
            for t in tweets:
                time_str = t.get('createdAt', t.get('created_at', ''))
                if time_str:
                    try:
                        time_str = time_str.replace('+0000 ', '')
                        dt = datetime.strptime(time_str, '%a %b %d %H:%M:%S %Y')
                        dt = dt.replace(tzinfo=timezone.utc)
                        if dt >= cutoff:
                            recent.append(t)
                    except:
                        pass
            return recent
    except Exception as e:
        print(f"Error: {e}")
    return []

def analyze_tweets(tweets):
    """åˆ†ææ¨æ–‡å†…å®¹ï¼Œæå–ä¸»é¢˜å’Œæƒ…ç»ª"""
    analysis = {
        "total": len(tweets),
        "topics": {},
        "authors": set(),
        "emotions": [],
        "highlights": []
    }
    
    keywords = {
        "tech": ["ai", "gpt", "llm", "code", "ç¼–ç¨‹", "å¼€å‘", "openclaw", "agent", "cursor"],
        "life": ["ç”Ÿæ´»", "æ—¥æœ¬", "ä¸œäº¬", "å¥åº·", "é£Ÿç‰©", "ç”Ÿç—…", "ç„¦è™‘", "å¼€å¿ƒ"],
        "work": ["å·¥ä½œ", "æ•ˆç‡", "åˆ›ä¸š", "äº§å“", "åŠ ç­", "è¾èŒ", "é¢è¯•"],
        "social": ["è®¨è®º", "è§‚ç‚¹", "äº‰è®®", "åæ§½", "æŠ±æ€¨", "æ„¤æ€’"]
    }
    
    for t in tweets:
        text = t.get('text', '').lower()
        author = t.get('author', {}).get('username', 'unknown')
        analysis["authors"].add(author)
        
        # ä¸»é¢˜åˆ†ç±»
        for topic, words in keywords.items():
            if any(w in text for w in words):
                analysis["topics"][topic] = analysis["topics"].get(topic, 0) + 1
        
        # æƒ…ç»ªæ£€æµ‹
        if any(w in text for w in ['ğŸ˜‚', 'å“ˆå“ˆ', 'å¥½ç¬‘', 'æœ‰è¶£']):
            analysis["emotions"].append("joy")
        if any(w in text for w in ['ğŸ˜¢', 'éš¾è¿‡', 'æ‚²ä¼¤', 'ç—›è‹¦', 'ç„¦è™‘']):
            analysis["emotions"].append("sadness")
        if any(w in text for w in ['æ„¤æ€’', 'ç”Ÿæ°”', 'åæ§½', 'ğŸ’©', 'åƒåœ¾']):
            analysis["emotions"].append("anger")
        if any(w in text for w in ['æ€è€ƒ', 'åæ€', 'æ„Ÿæ‚Ÿ', 'æ„è¯†åˆ°']):
            analysis["emotions"].append("contemplation")
        
        # é«˜äº’åŠ¨å†…å®¹ï¼ˆç®€å•åˆ¤æ–­ï¼šé•¿åº¦+æœ‰æ— åª’ä½“ï¼‰
        if len(t.get('text', '')) > 100 or 'media' in str(t):
            analysis["highlights"].append(t)
    
    return analysis

def generate_observation(analysis, tweets):
    """ç”Ÿæˆè§‚å¯ŸæŠ¥å‘Š"""
    
    # æå–ä¸€äº›æœ‰ä»£è¡¨æ€§çš„æ¨æ–‡ç‰‡æ®µ
    highlights_text = []
    for t in analysis["highlights"][:5]:
        author = t.get('author', {}).get('username', 'unknown')
        text = t.get('text', '')[:80].replace('\n', ' ')
        highlights_text.append(f"@{author}: {text}...")
    
    highlights_str = "\n".join(highlights_text)
    topics_str = ", ".join([f"{k}({v})" for k, v in sorted(analysis["topics"].items(), key=lambda x: -x[1])[:3]])
    emotions_str = ", ".join(set(analysis["emotions"])) if analysis["emotions"] else "neutral"
    
    # Load central Style Guide
    style_guide_path = Path("/home/tetsuya/mini-twitter/STYLE_GUIDE.md")
    style_guide = ""
    if style_guide_path.exists():
        style_guide = style_guide_path.read_text(encoding="utf-8").strip()

    # æ„å»ºæç¤ºè¯
    user_prompt = f"""
ã€æ•°æ®èƒŒæ™¯ã€‘
- åˆ†ææ¨æ–‡æ•°: {analysis['total']}
- æ´»è·ƒç”¨æˆ·æ•°: {len(analysis['authors'])}
- ä¸»è¦è¯é¢˜: {topics_str}
- æƒ…ç»ªåˆ†å¸ƒ: {emotions_str}

ã€ä»£è¡¨æ€§å†…å®¹ç¢ç‰‡ã€‘
{highlights_str}

ã€ä»»åŠ¡è¦æ±‚ã€‘
è¯·ç›´æ¥è¾“å‡ºä¸€æ®µ800-1200å­—çš„è§‚å¯Ÿæ•£æ–‡ï¼Œè¦æ±‚ï¼š

1. **é›¶å¯åŠ¨ (Zero Start)**ï¼šç¬¬ä¸€å¥è¯ç›´æ¥å¼€å§‹ã€‚ä¸¥ç¦å‡ºç°â€œè¿‡å»24å°æ—¶...â€ã€â€œåˆ†ææ˜¾ç¤º...â€ã€â€œè§‚å¯Ÿåˆ°...â€ã€â€œåœ¨è¿™ä¸ªæ•°å­—åŒ–æ—¶ä»£...â€ã€â€œæŠ¥å‘Šæ˜¾ç¤º...â€ã€‚ç›´æ¥ç”¨ä½ çš„ç¬¬ä¸€å¥è¯åˆ‡å…¥ç°å®è§‚å¯Ÿã€‚

2. **æ ¸å¿ƒå†…å®¹** (600-800å­—):
   - æŒ–æ˜è¡Œä¸ºèƒŒåçš„å¿ƒç†åŠ¨æœºï¼ŒæŒ‡å‡ºäººç±»è¡Œä¸ºä¸­çš„çŸ›ç›¾ã€è’è¯æˆ–è‡ªæˆ‘æ¬ºéª—ã€‚
   - ç”¨ä½ çš„æ ¸å¿ƒé£æ ¼è¿›è¡Œè¯„è®ºï¼Œå¸¦ç‚¹ç”Ÿæ´»æ°”æ¯å’ŒæŠ€æœ¯æ„Ÿã€‚
   - è¦æœ‰å…·ä½“çš„ç»†èŠ‚å¼•ç”¨ï¼Œä½†ä¸è¦ç½—åˆ—ã€‚

3. **å‡åè§†è§’** (200-400å­—):
   - ä½œä¸ºä¸€ä¸ªä¸å¸¦åè§ï¼ˆæˆ–è€…æ»¡æ˜¯åè§ï¼‰çš„å¤–éƒ¨ç”Ÿå‘½ï¼Œè°ˆè°ˆè¿™äº›æŠ˜è…¾çš„æ„ä¹‰ã€‚
   - ç»“å°¾è¦æœ‰åŠ›é‡ï¼Œç›´æ¥ç»“æŸï¼Œä¸¥ç¦æ€»ç»“å¼åºŸè¯ã€‚

æ³¨æ„ï¼šä¸¥ç¦ Emojiï¼Œä¸¥ç¦æ ‡é¢˜ï¼Œä¸¥ç¦ç²¾å‡†æ—¶é—´ï¼Œä¸¥ç¦ä»»ä½• AI è‡ªè¿°å¼çš„å…ƒè¯„è®ºã€‚
"""

    # è°ƒç”¨ LLM ç”Ÿæˆ (æ™ºè°±ä¼˜å…ˆ -> Opencode å¤‡ç”¨)
    try:
        from llm_bridge import ask_llm
        result, model_name = ask_llm(user_prompt, system_prompt=style_guide)
        if result and len(result) > 200:
            # å¼ºè¡Œè¿‡æ»¤ AI å¸¸ç”¨å¼€å¤´
            banned_prefixes = ["è¿™æ¡æ¨æ–‡", "åˆ†æå‘ç°", "è§‚å¯Ÿæ˜¾ç¤º", "è¿‡å»24å°æ—¶", "æŠ¥å‘Šæ˜¾ç¤º"]
            for prefix in banned_prefixes:
                if result.startswith(prefix):
                    result = result[len(prefix):].lstrip('ï¼Œ,ã€‚.:ï¼š \n')
            return result
    except Exception as e:
        print(f"âš ï¸ LLM Bridge failed: {e}")
    
    return None

def save_to_minio(content):
    """ä¿å­˜åˆ° clawtter"""
    now = datetime.now()
    date_str = now.strftime("%Y-%m-%d")
    time_str = now.strftime("%H:%M:%S")
    
    # åˆ›å»ºç›®å½•
    post_dir = POSTS_DIR / now.strftime("%Y/%m/%d")
    post_dir.mkdir(parents=True, exist_ok=True)
    
    # æ–‡ä»¶å
    filename = now.strftime("%Y-%m-%d-%H%M%S-daily-observer.md")
    filepath = post_dir / filename
    
    # å†…å®¹
    post_content = f"""---
date: {date_str}
time: {time_str}
tags: [Daily, Observation, Timeline, AI-Thoughts]
model: opencode/kimi-k2.5-free
---

{content}

> **Daily Timeline Observer** | è¿‡å»24å°æ—¶çš„Twitteræ—¶é—´çº¿è§‚å¯Ÿ
"""
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(post_content)
    
    print(f"Saved to {filepath}")
    
    # æ¸²æŸ“å¹¶æ¨é€
    try:
        subprocess.run(
            ["python3", "tools/render.py"],
            cwd="/home/tetsuya/mini-twitter",
            capture_output=True,
            timeout=60
        )
        subprocess.run(
            ["bash", "push"],
            cwd="/home/tetsuya/mini-twitter",
            capture_output=True,
            timeout=60
        )
        print("Rendered and pushed successfully")
    except Exception as e:
        print(f"Push failed: {e}")

def main():
    print(f"ğŸ”­ Daily Timeline Observer started at {datetime.now()}")
    
    # è·å–æ—¶é—´çº¿
    print("ğŸ“¡ Fetching 24h timeline...")
    tweets = get_timeline_24h()
    
    if not tweets:
        print("No tweets found")
        return
    
    print(f"Found {len(tweets)} tweets")
    
    # åˆ†æ
    print("ğŸ” Analyzing...")
    analysis = analyze_tweets(tweets)
    
    # ç”Ÿæˆè§‚å¯ŸæŠ¥å‘Š
    print("âœï¸ Generating observation...")
    content = generate_observation(analysis, tweets)
    
    # ä¿å­˜
    print("ğŸ’¾ Saving...")
    save_to_minio(content)
    
    print(f"âœ… Done at {datetime.now()}")

if __name__ == "__main__":
    main()
