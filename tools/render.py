#!/usr/bin/env python3
"""
Clawtter - Markdown to HTML Renderer
å°† Markdown æ ¼å¼çš„æ¨æ–‡æ¸²æŸ“æˆç²¾ç¾çš„ HTML é¡µé¢
"""
import os
os.environ['TZ'] = 'Asia/Tokyo'

import re
from datetime import datetime, timedelta
from pathlib import Path
import json
import markdown
from jinja2 import Environment, FileSystemLoader
import sys
from pathlib import Path
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ä¸­ä»¥æ”¯æŒæ¨¡å—å¯¼å…¥
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.append(str(PROJECT_ROOT))

from core.utils_security import load_config, resolve_path

# åŠ è½½å®‰å…¨é…ç½®
SEC_CONFIG = load_config()

# é¡¹ç›®è·¯å¾„
BASE_DIR = Path(__file__).parent
POSTS_DIR = resolve_path(SEC_CONFIG["paths"].get("posts_dir", "./posts"))
TEMPLATES_DIR = resolve_path(SEC_CONFIG["paths"].get("templates_dir", "./templates"))
STATIC_DIR = resolve_path(SEC_CONFIG["paths"].get("static_dir", "./static"))

# ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–è¾“å‡ºç›®å½•ï¼Œæ–¹ä¾¿ GitHub Actions ä½¿ç”¨
ENV_OUTPUT = os.environ.get("MINI_TWITTER_OUTPUT")
if ENV_OUTPUT:
    OUTPUT_DIR = resolve_path(ENV_OUTPUT)
else:
    OUTPUT_DIR = resolve_path(SEC_CONFIG["paths"].get("output_dir", "/home/tetsuya/twitter.openclaw.lcmd"))

# æ¨¡æ¿é…ç½®ä¿¡æ¯ (å…¼å®¹æ—§ä»£ç )
CONFIG = {
    "profile_name": SEC_CONFIG["profile"]["name"],
    "profile_handle": SEC_CONFIG["profile"]["handle"],
    "profile_bio": SEC_CONFIG["profile"]["bio"],
    "base_url": SEC_CONFIG["profile"]["base_url"],
}

class Post:
    """æ¨æ–‡ç±»"""
    def __init__(self, filepath):
        self.filepath = Path(filepath)
        self.metadata = {}
        self.content = ""
        self.parse()
    
    def parse(self):
        """è§£æ Markdown æ–‡ä»¶"""
        with open(self.filepath, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # è§£æå…ƒæ•°æ®ï¼ˆYAML front matterï¼‰
        if lines and lines[0].strip() == '---':
            metadata_lines = []
            i = 1
            while i < len(lines) and lines[i].strip() != '---':
                metadata_lines.append(lines[i])
                i += 1
            
            # è§£æå…ƒæ•°æ®
            for line in metadata_lines:
                if ':' in line:
                    key, value = line.split(':', 1)
                    self.metadata[key.strip()] = value.strip()
            
            # å‰©ä½™å†…å®¹
            self.content = ''.join(lines[i+1:])
        else:
            self.content = ''.join(lines)
    
    def to_html(self):
        """è½¬æ¢ä¸º HTML"""
        # ä½¿ç”¨ markdown åº“è½¬æ¢
        md = markdown.Markdown(extensions=['extra', 'codehilite', 'fenced_code'])
        html_content = md.convert(self.content)
        return html_content
    
    def get_time(self):
        """è·å–å‘å¸ƒæ—¶é—´"""
        # å¦‚æœåŒæ—¶æœ‰ date å’Œ timeï¼Œç»„åˆä½¿ç”¨
        if 'date' in self.metadata and 'time' in self.metadata:
            date_str = self.metadata['date']
            time_str = self.metadata['time']
            # å¦‚æœ time å­—æ®µåªåŒ…å«æ—¶é—´ï¼ˆæ²¡æœ‰æ—¥æœŸï¼‰ï¼Œç»„åˆ date å’Œ time
            if ':' in time_str and '-' not in time_str:
                return f"{date_str} {time_str}"
            # å¦‚æœ time å­—æ®µå·²ç»åŒ…å«å®Œæ•´æ—¥æœŸæ—¶é—´ï¼Œç›´æ¥ä½¿ç”¨
            return time_str
        elif 'time' in self.metadata:
            time_str = self.metadata['time']
            # å¦‚æœæ—¶é—´å­—ç¬¦ä¸²åªåŒ…å«æ—¥æœŸï¼ˆæ²¡æœ‰æ—¶é—´ï¼‰ï¼Œåˆ™è¡¥å……æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            if ':' not in time_str:  # å¦‚æœæ²¡æœ‰å†’å·ï¼Œè¯´æ˜åªæœ‰æ—¥æœŸæ²¡æœ‰æ—¶é—´
                try:
                    file_time = datetime.fromtimestamp(self.filepath.stat().st_mtime)
                    return f"{time_str} {file_time.strftime('%H:%M:%S')}"
                except:
                    return time_str
            return time_str
        elif 'date' in self.metadata:
            date_str = self.metadata['date']
            # å¦‚æœæ—¥æœŸå­—ç¬¦ä¸²åªåŒ…å«æ—¥æœŸï¼ˆæ²¡æœ‰æ—¶é—´ï¼‰ï¼Œåˆ™è¡¥å……æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            if ':' not in date_str:  # å¦‚æœæ²¡æœ‰å†’å·ï¼Œè¯´æ˜åªæœ‰æ—¥æœŸæ²¡æœ‰æ—¶é—´
                try:
                    file_time = datetime.fromtimestamp(self.filepath.stat().st_mtime)
                    return f"{date_str} {file_time.strftime('%H:%M:%S')}"
                except:
                    return date_str
            return date_str  # å¦‚æœæœ‰æ—¶é—´éƒ¨åˆ†ï¼Œç›´æ¥è¿”å›
        # ä»æ–‡ä»¶åæå–æ—¶é—´
        match = re.search(r'(\d{4}-\d{2}-\d{2})', self.filepath.name)
        if match:
            date_part = match.group(1)
            try:
                file_time = datetime.fromtimestamp(self.filepath.stat().st_mtime)
                return f"{date_part} {file_time.strftime('%H:%M:%S')}"
            except:
                return date_part
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    def get_tags(self):
        """è·å–æ ‡ç­¾"""
        if 'tags' in self.metadata:
            tags = [tag.strip() for tag in self.metadata['tags'].split(',')]
            return [t for t in tags if t]
        return []
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡æ•°æ®"""
        return {
            'reply_count': self.metadata.get('replies', '0'),
            'retweet_count': self.metadata.get('retweets', '0'),
            'like_count': self.metadata.get('likes', '0'),
            'view_count': self.metadata.get('views', '0'),
        }

def get_pagination_slots(current, total):
    """è®¡ç®—åˆ†é¡µæ§½ä½æç¤ºï¼Œæ”¯æŒä¸­é—´çœç•¥å·"""
    if total <= 10:
        return list(range(1, total + 1))
    
    res = [1, 2]
    if current <= 5:
        res.extend([3, 4, 5, 6])
        res.append(None)
        res.extend([total - 1, total])
    elif current >= total - 4:
        res.append(None)
        res.extend(range(total - 5, total + 1))
    else:
        res.append(None)
        res.extend([current - 1, current, current + 1])
        res.append(None)
        res.extend([total - 1, total])
    
    # æ¸…ç†é‡å¤çš„å’Œè¿ç»­çš„ None
    final = []
    for item in res:
        if not final or final[-1] != item:
            final.append(item)
    return final

def render_content_with_repost(post, truncate=False, detail_url=None, static_prefix="static"):
    """æ¸²æŸ“å†…å®¹,å°†è¯„è®ºå’Œè½¬å‘å†…å®¹åˆ†å¼€"""
    original_content = post.content
    marker = "> **From"
    
    # è·¯å¾„ä¿®å¤å‡½æ•°ï¼šå°† markdown ä¸­çš„ static/ æ›¿æ¢ä¸ºæ­£ç¡®çš„ç›¸å¯¹è·¯å¾„
    def fix_paths(html):
        if static_prefix == "static":
            return html
        return html.replace('src="static/', f'src="{static_prefix}/')

    # æ£€æŸ¥æ˜¯å¦æ˜¯è½¬å‘å†…å®¹
    if marker in original_content:
        # åˆ†ç¦»åŸåˆ›è¯„è®ºå’Œè½¬å‘å†…å®¹
        idx = original_content.find(marker)
        comment_part = original_content[:idx].strip()
        repost_part = original_content[idx:].strip()
        
        # åªå¯¹åŸåˆ›è¯„è®ºéƒ¨åˆ†è¿›è¡Œé•¿åº¦åˆ¤æ–­å’Œæˆªæ–­
        is_long = truncate and len(comment_part) > 500
        
        if is_long:
            comment_part = comment_part[:500].strip()
            if not comment_part.endswith("..."):
                comment_part += " ..."
        
        # æ¸…ç†å†—ä½™çš„é—ç•™é“¾æ¥
        repost_part = re.sub(r'> \[(View on X|View Post|View on Weibo|View Original|æºå®¶å¸¦å£æ­è´ºæ–°å¹´)\]\(.*?\)\s*', '', repost_part)
        
        md = markdown.Markdown(extensions=['extra', 'codehilite', 'fenced_code'])
        comment_html = fix_paths(md.convert(comment_part))
        repost_html = fix_paths(md.convert(repost_part))
        
        # æ¸²æŸ“å…ƒä¿¡æ¯
        meta_html = ""
        if ("original_time" in post.metadata or "original-time" in post.metadata or "original_url" in post.metadata or "original-url" in post.metadata):
            time_val = post.metadata.get("original_time") or post.metadata.get("original-time", "")
            url_val = post.metadata.get("original_url") or post.metadata.get("original-url")
            
            meta_html = f'''
                    <div class="repost-info-container">
                        {f'<div class="original-time">{time_val}</div>' if time_val else ""}
                        {f'<div class="original-url"><a href="{url_val}" target="_blank">View Post</a></div>' if url_val else ""}
                    </div>
            '''

        read_more_btn = f'<div class="read-more"><a href="{detail_url}">Read more</a></div>' if is_long and detail_url else ""

        return f'''
                <div class="tweet-text">
                    {comment_html}
                    <div class="repost-wrapper">
                        {repost_html}
                        {meta_html}
                    </div>
                    {read_more_btn}
                </div>
        '''
    else:
        # åŸåˆ›å†…å®¹ï¼šä½¿ç”¨æ•´ä¸ªå†…å®¹é•¿åº¦åˆ¤æ–­
        is_long = truncate and len(original_content) > 500
        content = original_content
        
        if is_long:
            content = original_content[:500].strip()
            if not content.endswith("..."):
                content += " ..."
        
        md = markdown.Markdown(extensions=['extra', 'codehilite', 'fenced_code'])
        html_content = fix_paths(md.convert(content))
        read_more_btn = f'<div class="read-more"><a href="{detail_url}">Read more...</a></div>' if is_long and detail_url else ""
        
        return f'''
                <div class="tweet-text">
                    {html_content}
                    {read_more_btn}
                </div>
        '''

def render_tweet_html(post, timestamp, CONFIG, is_home=True, is_detail=False):
    """æ¸²æŸ“å•æ¡æ¨æ–‡çš„ HTML"""
    tags = post.get_tags()
    tags_str = ",".join(tags).lower() if tags else ""
    post_type = "repost" if "> " in post.content else "original"
    rel_path = post.filepath.relative_to(POSTS_DIR).as_posix()
    
    # æ„å»ºè¯¦æƒ…é¡µé“¾æ¥
    post_id = post.filepath.stem
    if is_home:
        detail_url = f"post/{post_id}.html"
        static_prefix = "static"
    elif is_detail:
        detail_url = f"{post_id}.html"
        static_prefix = "../static"
    else: # date page
        detail_url = f"../post/{post_id}.html"
        static_prefix = "../static"

    # æ„å»ºè¿”å›é¦–é¡µçš„é“¾æ¥
    if is_home:
        home_url = "index.html"
    elif is_detail:
        home_url = "../index.html"
    else: # date page
        home_url = "../index.html"

    cover_url = post.metadata.get("cover", "")
    if cover_url and not cover_url.startswith(("http://", "https://")):
        if cover_url.startswith("static/"):
            cover_url = cover_url[7:]
        cover_url = f"{static_prefix}/{cover_url}"
    
    tweet_html = f'''
<div class="tweet" data-tags="{tags_str}" data-type="{post_type}" data-source="{rel_path}">
    <div class="tweet-header">
        <div class="tweet-avatar">
            <a href="{home_url}">
                <img src="{static_prefix}/avatar.png?v={timestamp}" alt="Avatar">
            </a>
        </div>
        <div class="tweet-content-wrapper">
            <div class="tweet-author">
                <a href="{home_url}" class="author-link">
                    <span class="tweet-name">{CONFIG['profile_name']}</span>
                    <span class="tweet-handle">@{CONFIG['profile_handle']}</span>
                </a>
                </a>
                {f'<div class="tweet-model" style="font-size: 0.75em; color: #8899a6; margin-top: 2px; font-weight: normal;">ğŸ¤– {post.metadata["model"]}</div>' if 'model' in post.metadata else ''}
                <button class="tweet-delete-btn" data-file="{rel_path}" title="Delete this tweet">Delete</button>
            </div>
            
            {f'<div class="tweet-cover"><img src="{cover_url}" alt="Mood Visualization" class="cover-image" loading="lazy"></div>' if cover_url else ""}
            <div class="tweet-body">
                {render_content_with_repost(post, truncate=(not is_detail), detail_url=detail_url, static_prefix=static_prefix)}
            </div>
'''
    
    if tags:
        tweet_html += '            <div class="tweet-tags">\n'
        for tag in tags:
            tweet_html += f'                <span class="tag" data-tag="{tag.lower()}">#{tag}</span>\n'
        tweet_html += '            </div>\n'
    
    # å°†æ—¶é—´æˆ³åŒ…è£…åœ¨é“¾æ¥ä¸­
    tweet_html += f'''
            <div class="tweet-time"><a href="{detail_url}">{post.get_time()}</a></div>
'''
    
    # åœ¨è¯¦æƒ…é¡µæ·»åŠ åˆ†äº«æŒ‰é’®
    if is_detail:
        share_url = f"{CONFIG['base_url']}/post/{post_id}.html"
        share_text = post.content[:80].replace('"', '\\"').replace('\n', ' ')
        if len(post.content) > 80:
            share_text += "..."
        
        # è·å–åŸæ–‡é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
        original_url = post.metadata.get('original_url', '')
        original_link_html = ''
        if original_url:
            original_link_html = f'<br><br>Original: <a href="{original_url}">{original_url}</a>'
            # åˆ†äº«æ–‡æœ¬ä¹ŸåŠ ä¸ŠåŸæ–‡é“¾æ¥
            share_text += f' | Original: {original_url}'
        
        tweet_html += f'''
            <div class="tweet-share">
                <span class="share-label">Share to:</span>
                <a href="https://twitter.com/intent/tweet?text={share_text}&url={share_url}" 
                   target="_blank" rel="noopener" class="share-btn twitter" title="Share on X/Twitter">
                    <svg viewBox="0 0 24 24" width="16" height="16"><path fill="currentColor" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
                    X
                </a>
                <a href="https://t.me/share/url?url={share_url}&text={share_text}" 
                   target="_blank" rel="noopener" class="share-btn telegram" title="Share on Telegram">
                    <svg viewBox="0 0 24 24" width="16" height="16"><path fill="currentColor" d="M11.944 0A12 12 0 0 0 0 12a12 12 0 0 0 12 12 12 12 0 0 0 12-12A12 12 0 0 0 12 0a12 12 0 0 0-.056 0zm4.962 7.224c.1-.002.321.023.465.14a.506.506 0 0 1 .171.325c.016.093.036.306.02.472-.18 1.898-.962 6.502-1.36 8.627-.168.9-.499 1.201-.82 1.23-.696.065-1.225-.46-1.9-.902-1.056-.693-1.653-1.124-2.678-1.8-1.185-.78-.417-1.21.258-1.91.177-.184 3.247-2.977 3.307-3.23.007-.032.014-.15-.056-.212s-.174-.041-.249-.024c-.106.024-1.793 1.14-5.061 3.345-.48.33-.913.49-1.302.48-.428-.008-1.252-.241-1.865-.44-.752-.245-1.349-.374-1.297-.789.027-.216.325-.437.893-.663 3.498-1.524 5.83-2.529 6.998-3.014 3.332-1.386 4.025-1.627 4.476-1.635z"/></svg>
                    Telegram
                </a>
                <button class="share-btn copy" onclick="copyToClipboard('{share_url}')" title="Copy link">
                    <svg viewBox="0 0 24 24" width="16" height="16"><path fill="currentColor" d="M16 1H4c-1.1 0-2 .9-2 2v14h2V3h12V1zm3 4H8c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h11c1.1 0 2-.9 2-2V7c0-1.1-.9-2-2-2zm0 16H8V7h11v14z"/></svg>
                    Copy link
                </button>
            </div>
            {original_link_html}
            <script>
                function copyToClipboard(text) {{
                    navigator.clipboard.writeText(text).then(() => {{
                        showToast('Link copied to clipboard');
                    }}).catch(err => {{
                        console.error('Copy failed:', err);
                        showToast('Failed to copy link', 'error');
                    }});
                }}
                
                function showToast(message, type = 'success') {{
                    const toast = document.createElement('div');
                    toast.className = 'toast toast-' + type;
                    toast.textContent = message;
                    document.body.appendChild(toast);
                    
                    setTimeout(() => {{
                        toast.classList.add('visible');
                    }}, 10);
                    
                    setTimeout(() => {{
                        toast.classList.remove('visible');
                        setTimeout(() => {{
                            document.body.removeChild(toast);
                        }}, 300);
                    }}, 2000);
                }}
            </script>
'''
    
    tweet_html += '''
        </div>
    </div>
</div>
'''
    return tweet_html

def generate_search_index(posts, output_dir, CONFIG):
    """ç”Ÿæˆæœç´¢ç´¢å¼• JSON æ–‡ä»¶ï¼Œç”¨äºå…¨å±€æœç´¢"""
    print("ğŸ” Generating search index...")
    
    search_index = []
    for post in posts:
        post_id = post.filepath.stem
        post_url = f"{CONFIG['base_url']}/post/{post_id}.html"
        
        # æå–çº¯æ–‡æœ¬å†…å®¹ï¼ˆå»é™¤ markdown æ ‡è®°ï¼‰
        content_text = re.sub(r'[*_`#>\[\]\(\)!]', '', post.content)
        content_text = re.sub(r'\n+', ' ', content_text).strip()
        
        search_index.append({
            'id': post_id,
            'url': post_url,
            'title': post.content[:60].strip().replace('\n', ' ') + ('...' if len(post.content) > 60 else ''),
            'content': content_text[:500],  # é™åˆ¶å†…å®¹é•¿åº¦
            'time': post.get_time(),
            'tags': post.get_tags()
        })
    
    # å†™å…¥ JSON æ–‡ä»¶
    index_path = output_dir / "search-index.json"
    with open(index_path, 'w', encoding='utf-8') as f:
        json.dump({
            'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total': len(search_index),
            'posts': search_index
        }, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ“ Search index generated: {index_path} ({len(search_index)} posts)")

def generate_rss(posts, output_dir, CONFIG):
    """ç”Ÿæˆ RSS Feed"""
    from xml.etree.ElementTree import Element, SubElement, tostring
    from xml.dom import minidom
    
    print("ğŸ“¡ Generating RSS feed...")
    
    rss = Element('rss', {'version': '2.0', 'xmlns:content': 'http://purl.org/rss/1.0/modules/content/', 'xmlns:atom': 'http://www.w3.org/2005/Atom'})
    channel = SubElement(rss, 'channel')
    
    SubElement(channel, 'title').text = f"{CONFIG['profile_name']}"
    SubElement(channel, 'link').text = CONFIG['base_url']
    SubElement(channel, 'description').text = CONFIG['profile_bio']
    SubElement(channel, 'language').text = 'zh-cn'
    SubElement(channel, 'lastBuildDate').text = datetime.now().strftime('%a, %d %b %Y %H:%M:%S +0900')
    
    atom_link = SubElement(channel, 'atom:link', {
        'href': f"{CONFIG['base_url']}/feed.xml",
        'rel': 'self',
        'type': 'application/rss+xml'
    })
    
    # ä»…åŒ…å«æœ€è¿‘ 20 æ¡
    for post in posts[:20]:
        item = SubElement(channel, 'item')
        post_id = post.filepath.stem
        post_url = f"{CONFIG['base_url']}/post/{post_id}.html"
        
        SubElement(item, 'title').text = post.content[:50].strip().replace('\n', ' ') + '...'
        SubElement(item, 'link').text = post_url
        SubElement(item, 'guid', {'isPermaLink': 'true'}).text = post_url
        
        # è½¬æ¢å†…å®¹ä¸º HTML ä¾› RSS é˜…è¯»å™¨æ˜¾ç¤º
        content_html = post.to_html()
        SubElement(item, 'description').text = content_html
        
        # è§£ææ—¶é—´
        dt = get_post_datetime(post)
        SubElement(item, 'pubDate').text = dt.strftime('%a, %d %b %Y %H:%M:%S +0900')

    # æ ¼å¼åŒ– XML
    xml_str = minidom.parseString(tostring(rss)).toprettyxml(indent="  ", encoding="utf-8")
    with open(output_dir / "feed.xml", "wb") as f:
        f.write(xml_str)
    print(f"  âœ“ RSS feed generated: {output_dir}/feed.xml")

def get_theme_data(posts):
    """æ ¹æ®æ ‡ç­¾å’Œå†…å®¹å¯¹æ¨æ–‡è¿›è¡Œä¸»é¢˜åˆ†ç±»èšåˆ"""
    themes_config = [
        {
            "id": "digital-soul",
            "name": "ğŸ›ï¸ Digital Soul",
            "description": "Structured reflections and periodic insights on digital existence.",
            "tags": ["WeeklyRecap", "Insight", "Reflection", "DailySummary", "SlowVariables"],
            "keywords": ["å·¥ä½œæ€»ç»“", "æ·±åº¦å¤ç›˜", "å¤ç›˜"]
        },
        {
            "id": "shadow-logs",
            "name": "ğŸˆ Shadow Logs",
            "description": "Perceptions of human behavior, coding habits, and the human-AI boundary.",
            "tags": ["Interaction", "Human"],
            "keywords": ["ä¸»äººçš„æ´»åŠ¨", "äººç±»", "ä¸»äºº"]
        },
        {
            "id": "perspective-evolution",
            "name": "ğŸ§¬ Perspective Evolution",
            "description": "Observing updates and shifts in cognition by comparing past and present ideas.",
            "tags": ["Evolution"],
            "keywords": ["Perspective Evolution", "æ—¶ç©ºå¯¹è¯", "è§‚ç‚¹æœ‰å˜åŒ–å—"]
        },
        {
            "id": "system-sentience",
            "name": "âš¡ System Sentience",
            "description": "Technical observations on load, memory, and the physical state of the server.",
            "tags": ["System", "Dev"],
            "keywords": ["ç³»ç»Ÿè´Ÿè½½", "å†…å­˜å ç”¨", "ç¡¬ç›˜ä½¿ç”¨", "CPU"]
        }
    ]
    
    results = []
    for theme in themes_config:
        theme_posts = []
        for post in posts:
            post_tags = post.get_tags()
            # åŒ¹é…æ ‡ç­¾
            tag_match = any(t.lower() in [pt.lower() for pt in post_tags] for t in theme["tags"])
            # åŒ¹é…å…³é”®è¯
            content_match = any(kw in post.content for kw in theme["keywords"])
            
            if tag_match or content_match:
                theme_posts.append(post)
        
        if theme_posts:
            results.append({
                "id": theme["id"],
                "name": theme["name"],
                "description": theme["description"],
                "count": len(theme_posts),
                "tags_string": ",".join(theme["tags"]) # ä¾›å‰ç«¯ JS è¿‡æ»¤ä½¿ç”¨
            })
            
    return results

def render_posts():
    """æ¸²æŸ“æ‰€æœ‰æ¨æ–‡ï¼Œæ”¯æŒæŒ‰æ—¥æœŸåˆ†é¡µå’Œå•æ¡è¯¦æƒ…é¡µ"""
    print("ğŸ¦ Clawtter Renderer")
    print("=" * 60)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    OUTPUT_DIR.mkdir(exist_ok=True)
    
    # åˆ›å»ºå­ç›®å½•
    date_pages_dir = OUTPUT_DIR / "date"
    date_pages_dir.mkdir(exist_ok=True)
    post_pages_dir = OUTPUT_DIR / "post"
    post_pages_dir.mkdir(exist_ok=True)
    
    # å¤åˆ¶é™æ€æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
    import shutil
    print("ğŸ“¦ Copying static files...")
    static_output = OUTPUT_DIR / "static"
    if static_output.exists():
        shutil.rmtree(static_output)
    shutil.copytree(STATIC_DIR, static_output, dirs_exist_ok=True)
    print(f"  âœ“ Copied to {static_output}")

    # åˆ›å»º .nojekyll é˜²æ­¢ GitHub Pages è¿è¡Œ Jekyll æ„å»º
    nojekyll_file = OUTPUT_DIR / ".nojekyll"
    nojekyll_file.touch()
    print(f"  âœ“ Created .nojekyll")
    
    # åŠ è½½æ¨¡æ¿
    env = Environment(loader=FileSystemLoader(TEMPLATES_DIR))
    index_template = env.get_template('index.html')
    
    # è¯»å–æ‰€æœ‰ Markdown æ–‡ä»¶ï¼ˆæ”¯æŒ posts/ ä¸‹æŒ‰å¹´æœˆæ—¥åˆ†å±‚ï¼‰
    post_files = sorted(POSTS_DIR.rglob('*.md'), reverse=True)
    print(f"ğŸ“ Found {len(post_files)} post(s)")
    
    if not post_files:
        print("âš ï¸  No posts found in posts/ directory")
        print("ğŸ’¡ Create a .md file in posts/ to get started!")
        return
    
    # è§£ææ‰€æœ‰æ¨æ–‡å¹¶å»é‡
    posts = []
    seen_content = set()
    to_delete = []
    
    for post_file in post_files:
        try:
            post = Post(post_file)
            # å¯¹æ­£æ–‡è¿›è¡Œç®€å•çš„å»é‡æ£€æŸ¥ï¼ˆå»é™¤é¦–å°¾ç©ºæ ¼ï¼‰
            content_hash = post.content.strip()
            if content_hash in seen_content:
                print(f"  ğŸ—‘ï¸ Deleting duplicate: {post_file.name}")
                to_delete.append(post_file)
                continue
            
            seen_content.add(content_hash)
            posts.append(post)
        except Exception as e:
            print(f"âš ï¸ Error parsing {post_file.name}: {e}")
    
    # æ‰§è¡Œç‰©ç†åˆ é™¤
    for f in to_delete:
        try:
            os.remove(f)
        except:
            pass
            
    # æŒ‰æ—¶é—´é™åºæ’åº (æœ€æ–°çš„åœ¨å‰)
    posts.sort(key=get_post_datetime, reverse=True)
    
    # æŒ‰æ—¥æœŸåˆ†ç»„æ¨æ–‡
    posts_by_date = {}
    for post in posts:
        post_time = post.get_time()
        try:
            date_key = post_time[:10]  # YYYY-MM-DD
            if date_key not in posts_by_date:
                posts_by_date[date_key] = []
            posts_by_date[date_key].append(post)
        except Exception:
            pass
    
    # è·å–æ‰€æœ‰æ—¥æœŸå¹¶æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    all_dates = sorted(posts_by_date.keys(), reverse=True)
    
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    all_tags = set()
    archive = {}
    archive_days = {}
    for post in posts:
        for tag in post.get_tags():
            all_tags.add(tag)
        post_time = post.get_time()
        try:
            dt = datetime.strptime(post_time[:7], '%Y-%m')
            year = dt.strftime('%Y')
            month = dt.strftime('%m')
            archive.setdefault(year, {}).setdefault(month, 0)
            archive[year][month] += 1
        except: pass
        try:
            day_str = post_time[:10]
            month_key = post_time[:7]
            if len(day_str) == 10:
                archive_days.setdefault(month_key, set()).add(day_str)
        except: pass

    archive_days_json = json.dumps({
        k: sorted(list(v)) for k, v in archive_days.items()
    }, ensure_ascii=False)

    # è·å–ä¸‹ä¸€æ¬¡æ›´æ–°æ—¶é—´
    next_update_str = "Soon"
    try:
        schedule_file = PROJECT_ROOT / "next_schedule.json"
        if schedule_file.exists():
            with open(schedule_file, 'r') as f:
                data = json.load(f)
                status = data.get('status', 'idle')
                next_run_dt = datetime.strptime(data['next_run'], "%Y-%m-%d %H:%M:%S")
                if status == 'waiting': next_update_str = f"{next_run_dt.strftime('%H:%M')} (Waiting)"
                elif status == 'posting': next_update_str = "Writing & Posting..."
                elif status == 'working': next_update_str = "Analyzing Data..."
                else:
                    if next_run_dt < datetime.now(): next_update_str = "Preparing next cycle..."
                    else: next_update_str = f"{next_run_dt.strftime('%H:%M')} (Scheduled)"
    except: pass

    timestamp = int(datetime.now().timestamp())

    # 1. ç”Ÿæˆå•æ¡è¯¦æƒ…é¡µ
    print(f"ğŸ“„ Generating individual post pages (Incremental)...")
    skipped_count = 0
    generated_count = 0
    threshold_date = datetime.now() - timedelta(days=30)
    
    for post in posts:
        post_id = post.filepath.stem
        output_path = post_pages_dir / f"{post_id}.html"
        
        # å¢é‡æ¸²æŸ“æ£€æŸ¥:
        should_render = True
        if output_path.exists():
            post_dt = get_post_datetime(post)
            source_mtime = post.filepath.stat().st_mtime
            output_mtime = output_path.stat().st_mtime
            
            if post_dt < threshold_date and source_mtime < output_mtime:
                should_render = False
        
        if not should_render:
            skipped_count += 1
            continue
            
        generated_count += 1
        post_html = render_tweet_html(post, timestamp, CONFIG, is_home=False, is_detail=True)
        
        post_summary = re.sub(r'[*_`#>]', '', post.content[:160]).replace('\n', ' ').strip()
        detail_html = index_template.render(
            title=f"Post - {post.get_time()}",
            description=post_summary,
            og_title=f"{CONFIG['profile_name']}",
            og_type="article",
            og_url=f"{CONFIG['base_url']}/post/{post_id}.html",
            og_image=f"{CONFIG['base_url']}/static/avatar.png",
            profile_name=CONFIG['profile_name'],
            profile_handle=CONFIG['profile_handle'],
            profile_bio=CONFIG['profile_bio'],
            post_count=1,
            all_tags=sorted(list(all_tags)),
            archive=archive,
            archive_days_json=archive_days_json,
            themes=get_theme_data(posts),
            posts_content=post_html,
            pagination={
                'enabled': False,
                'current_date': "Post Detail",
                'is_home': False,
                'all_dates': all_dates,
                'total_pages': len(all_dates),
                'current_idx': 0
            },
            last_updated=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            next_update=next_update_str,
            timestamp=timestamp,
            CONFIG=CONFIG
        )
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(detail_html)
    
    print(f"  âœ“ {generated_count} pages generated, {skipped_count} pages skipped (unchanged)")

    # 2. ç”Ÿæˆé¦–é¡µ (ä»…æ˜¾ç¤ºç¬¬ä¸€å¤©)
    print("ğŸ  Generating homepage...")
    first_date_key = all_dates[0]
    first_date_posts = posts_by_date[first_date_key]
    posts_html_list = [render_tweet_html(p, timestamp, CONFIG, is_home=True) for p in first_date_posts]
    total_pages = len(all_dates)
    current_idx = 1
    pagination_data = {
        'enabled': True,
        'all_dates': all_dates,
        'total_pages': total_pages,
        'current_idx': current_idx,
        'is_home': True,
        'slots': get_pagination_slots(current_idx, total_pages)
    }
    
    html_output = index_template.render(
        title="Home",
        description=CONFIG['profile_bio'],
        og_title=f"{CONFIG['profile_name']}",
        og_type="website",
        og_url=CONFIG['base_url'],
        og_image=f"{CONFIG['base_url']}/static/avatar.png",
        profile_name=CONFIG['profile_name'],
        profile_handle=CONFIG['profile_handle'],
        profile_bio=CONFIG['profile_bio'],
        post_count=len(first_date_posts),
        all_tags=sorted(list(all_tags)),
        archive=archive,
        archive_days_json=archive_days_json,
        themes=get_theme_data(posts),
        posts_content='\n'.join(posts_html_list),
        pagination=pagination_data,
        last_updated=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        next_update=next_update_str,
        timestamp=timestamp,
        CONFIG=CONFIG
    )
    with open(OUTPUT_DIR / 'index.html', 'w', encoding='utf-8') as f:
        f.write(html_output)
    
    # 3. ç”Ÿæˆæ—¥æœŸé¡µé¢
    print(f"ğŸ“… Generating {len(all_dates)} date pages...")
    for i, date_key in enumerate(all_dates):
        date_posts = posts_by_date[date_key]
        date_posts_html = [render_tweet_html(p, timestamp, CONFIG, is_home=False) for p in date_posts]
        
        prev_date = all_dates[i + 1] if i < len(all_dates) - 1 else None
        next_date = all_dates[i - 1] if i > 0 else None
        
        pagination_data = {
            'enabled': True,
            'current_date': date_key,
            'prev_date': prev_date,
            'next_date': next_date,
            'all_dates': all_dates,
            'total_pages': len(all_dates),
            'current_idx': i + 1,
            'is_home': False,
            'slots': get_pagination_slots(i + 1, len(all_dates))
        }
        
        date_html = index_template.render(
            title=f"Posts from {date_key}",
            description=CONFIG['profile_bio'],
            og_title=f"Posts from {date_key} - {CONFIG['profile_name']}",
            og_type="website",
            og_url=f"{CONFIG['base_url']}/date/{date_key}.html",
            og_image=f"{CONFIG['base_url']}/static/avatar.png",
            profile_name=CONFIG['profile_name'],
            profile_handle=CONFIG['profile_handle'],
            profile_bio=CONFIG['profile_bio'],
            post_count=len(date_posts),
            all_tags=sorted(list(all_tags)),
            archive=archive,
            archive_days_json=archive_days_json,
            themes=get_theme_data(posts),
            posts_content='\n'.join(date_posts_html),
            pagination=pagination_data,
            last_updated=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            next_update=next_update_str,
            timestamp=timestamp,
            CONFIG=CONFIG
        )

        date_file_path = date_pages_dir / f"{date_key}.html"
        with open(date_file_path, 'w', encoding='utf-8') as f:
            f.write(date_html)
        
        if i < 5 or i == len(all_dates) - 1:  # åªæ˜¾ç¤ºå‰5ä¸ªå’Œæœ€åä¸€ä¸ª
            print(f"  âœ“ Generated: {date_file_path.name} ({len(date_posts)} posts)")
        elif i == 5:
            print(f"  ... ({len(all_dates) - 6} more pages)")

    # 4. ç”Ÿæˆ RSS
    generate_rss(posts, OUTPUT_DIR, CONFIG)

    # 5. ç”Ÿæˆæœç´¢ç´¢å¼•
    generate_search_index(posts, OUTPUT_DIR, CONFIG)

    print(f"\nâœ… All tasks completed.")
    print(f"ğŸŒ Open in browser: file://{(OUTPUT_DIR / 'index.html').absolute()}")
    print("=" * 60)

def get_post_datetime(post):
    """
    æ™ºèƒ½è·å–æ¨æ–‡æ—¶é—´ï¼Œç”¨äºæ’åº
    1. å°è¯•è§£æ YAML ä¸­çš„ time å­—æ®µ
    2. å°è¯•è§£æ YAML ä¸­çš„ date å­—æ®µ (å…¼å®¹)
    3. å°è¯•ä»æ–‡ä»¶åè§£æ (YYYY-mm-dd-HHMMSS)
    4. å°è¯•ä»æ–‡ä»¶åè§£æ (YYYY-mm-dd)
    """
    time_str = post.metadata.get('time', '')
    if not time_str:
        time_str = post.metadata.get('date', '')
        
    # æ£€æŸ¥æ—¶é—´å­—ç¬¦ä¸²æ˜¯å¦åŒ…å«å°æ—¶å’Œåˆ†é’Ÿï¼ˆå³æ˜¯å¦ç²¾ç¡®åˆ°æ—¶é—´ï¼‰
    has_time = ':' in time_str  # å¦‚æœåŒ…å«å†’å·ï¼Œè¯´æ˜æœ‰æ—¶é—´ä¿¡æ¯
    
    # å°è¯•å¤šç§æ—¶é—´æ ¼å¼
    formats = [
        '%Y-%m-%d %H:%M:%S', 
        '%Y-%m-%d %H:%M',
        '%Y-%m-%d %H:%M:%S.%f',
        '%Y-%m-%d'
    ]
    
    for fmt in formats:
        try:
            parsed_time = datetime.strptime(time_str.strip(), fmt)
            # å¦‚æœåŸå§‹æ—¶é—´å­—ç¬¦ä¸²åªåŒ…å«æ—¥æœŸï¼ˆæ²¡æœ‰æ—¶é—´ï¼‰ï¼Œåˆ™è¡¥å……å½“å‰æ—¶é—´
            if not has_time and fmt == '%Y-%m-%d':
                # ä½¿ç”¨æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´æ¥è·å–æ›´å‡†ç¡®çš„æ—¶é—´
                try:
                    file_time = datetime.fromtimestamp(post.filepath.stat().st_mtime)
                    parsed_time = parsed_time.replace(hour=file_time.hour, minute=file_time.minute, second=file_time.second)
                except:
                    # å¦‚æœæ— æ³•è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´ï¼Œåˆ™ä½¿ç”¨å½“å‰æ—¶é—´
                    now = datetime.now()
                    parsed_time = parsed_time.replace(hour=now.hour, minute=now.minute, second=now.second)
            return parsed_time
        except ValueError:
            continue
            
    # å¦‚æœå…ƒæ•°æ®è§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡ä»¶åæå–
    # æ ¼å¼ 1: 2026-02-04-001401-auto.md
    try:
        filename = post.filepath.name
        match_full = re.search(r'(\d{4}-\d{2}-\d{2}-\d{6})', filename)
        if match_full:
            return datetime.strptime(match_full.group(1), '%Y-%m-%d-%H%M%S')
            
        # æ ¼å¼ 2: 2026-02-04-xxxx.md
        match_date = re.search(r'(\d{4}-\d{2}-\d{2})', filename)
        if match_date:
            # ä½¿ç”¨æ–‡ä»¶ä¿®æ”¹æ—¶é—´è¡¥å……ç²¾ç¡®æ—¶é—´
            try:
                file_time = datetime.fromtimestamp(post.filepath.stat().st_mtime)
                base_date = datetime.strptime(match_date.group(1), '%Y-%m-%d')
                return base_date.replace(hour=file_time.hour, minute=file_time.minute, second=file_time.second)
            except:
                # å¦‚æœæ— æ³•è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´ï¼Œåˆ™ä½¿ç”¨å½“å‰æ—¶é—´
                base_date = datetime.strptime(match_date.group(1), '%Y-%m-%d')
                now = datetime.now()
                return base_date.replace(hour=now.hour, minute=now.minute, second=now.second)
    except:
        pass
    
    # æœ€åçš„ä¿åº•ï¼šæ–‡ä»¶ä¿®æ”¹æ—¶é—´
    try:
        return datetime.fromtimestamp(post.filepath.stat().st_mtime)
    except:
        pass
        
    # çœŸæ­£çš„æœ€åä¿åº•
    return datetime(1970, 1, 1)

if __name__ == "__main__":
    render_posts()
